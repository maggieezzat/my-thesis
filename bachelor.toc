\contentsline {chapter}{Acknowledgments}{V}{chapter*.1}
\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}
\contentsline {chapter}{\numberline {2}Literature Review}{3}{chapter.2}
\contentsline {section}{\numberline {2.1}Natural Language Processing}{3}{section.2.1}
\contentsline {section}{\numberline {2.2}Artificial Neural Networks}{3}{section.2.2}
\contentsline {subsection}{\numberline {2.2.1}Feed Forward Neural Networks}{4}{subsection.2.2.1}
\contentsline {subsubsection}{Sequence Models and the Problem with \ac {FNN}s}{5}{section*.14}
\contentsline {subsection}{\numberline {2.2.2}Recurrent Neural Networks}{5}{subsection.2.2.2}
\contentsline {subsubsection}{Bidirectional \ac {RNN}s}{6}{section*.18}
\contentsline {subsubsection}{Problems with \ac {RNN}s: Vanishing and Exploding Gradients}{6}{section*.21}
\contentsline {subsubsection}{Long Short-Term Memory}{7}{section*.22}
\contentsline {section}{\numberline {2.3}Automatic Speech Recognition}{7}{section.2.3}
\contentsline {subsection}{\numberline {2.3.1}HMM-Based ASR Systems}{8}{subsection.2.3.1}
\contentsline {subsubsection}{Feature Extraction}{9}{section*.28}
\contentsline {subsubsection}{Acoustic Model}{10}{section*.33}
\contentsline {subsubsection}{Language Model}{11}{section*.35}
\contentsline {subsection}{\numberline {2.3.2}End-to-End Systems}{12}{subsection.2.3.2}
\contentsline {subsection}{\numberline {2.3.3}Deep Speech 2: End-to-End Speech Recognition Model}{13}{subsection.2.3.3}
\contentsline {subsubsection}{Sorta Grad}{14}{section*.39}
\contentsline {subsubsection}{Batch Normalization}{14}{section*.40}
\contentsline {subsubsection}{Look-ahead Convolution}{14}{section*.41}
\contentsline {section}{\numberline {2.4}Text Analysis}{14}{section.2.4}
\contentsline {subsection}{\numberline {2.4.1}Encoder-Decoder Architecture}{15}{subsection.2.4.1}
\contentsline {subsection}{\numberline {2.4.2}Attention Mechanism}{16}{subsection.2.4.2}
\contentsline {subsubsection}{\MakeUppercase {i}. Encoder}{16}{section*.46}
\contentsline {subsubsection}{\MakeUppercase {ii}. Decoder}{17}{section*.47}
\contentsline {subsection}{\numberline {2.4.3}The Transformer}{18}{subsection.2.4.3}
\contentsline {subsubsection}{Problem with Recurrence}{18}{section*.49}
\contentsline {subsubsection}{I. Encoder}{18}{section*.50}
\contentsline {subsubsection}{II. Decoder}{19}{section*.52}
\contentsline {subsection}{\numberline {2.4.4}Bidirectional Encoder from Transformer (BERT)}{20}{subsection.2.4.4}
\contentsline {subsubsection}{Model Architecture and Input Representation}{20}{section*.53}
\contentsline {subsubsection}{Pre-training Tasks}{20}{section*.55}
\contentsline {chapter}{\numberline {3}Methodology}{23}{chapter.3}
\contentsline {section}{\numberline {3.1}System Overview}{24}{section.3.1}
\contentsline {subsection}{\numberline {3.1.1}Two sub-systems vs. One speech-based End-to-End System}{24}{subsection.3.1.1}
\contentsline {section}{\numberline {3.2}Automatic Speech Recognition Unit}{24}{section.3.2}
\contentsline {subsection}{\numberline {3.2.1}Hybrid Kaldi-based ASR Model}{24}{subsection.3.2.1}
\contentsline {subsection}{\numberline {3.2.2}Why End-to-End ASR?}{25}{subsection.3.2.2}
\contentsline {subsection}{\numberline {3.2.3}Deep Speech 2: Adapting to German}{25}{subsection.3.2.3}
\contentsline {subsection}{\numberline {3.2.4}Speech Recognition Data}{25}{subsection.3.2.4}
\contentsline {subsubsection}{\MakeUppercase {i}. Common Voice}{25}{section*.62}
\contentsline {subsubsection}{\MakeUppercase {ii}. M-AILABS Speech Dataset}{26}{section*.63}
\contentsline {subsubsection}{\MakeUppercase {iii}. German Speech Data (Tuda-De)}{26}{section*.64}
\contentsline {subsubsection}{\MakeUppercase {iv}. CSS10: Single Speaker Data}{26}{section*.65}
\contentsline {subsubsection}{\MakeUppercase {v}. Movies Data}{26}{section*.66}
\contentsline {subsubsection}{\MakeUppercase {v}. Spoken Wikipedia Corpus}{27}{section*.68}
\contentsline {subsubsection}{Transcriptions Cleaning}{28}{section*.70}
\contentsline {section}{\numberline {3.3}Deep Speech 2: TensorFlow}{28}{section.3.3}
\contentsline {subsection}{\numberline {3.3.1}TensorFlow GPU}{28}{subsection.3.3.1}
\contentsline {subsubsection}{Implementation Details: Estimators}{28}{section*.73}
\contentsline {subsubsection}{Training Data, Process and Results}{29}{section*.74}
\contentsline {subsection}{\numberline {3.3.2}TensorFlow TPU}{29}{subsection.3.3.2}
\contentsline {subsubsection}{Migrating from Regular Estimator to TPU Estimator}{29}{section*.77}
\contentsline {section}{\numberline {3.4}Deep Speech 2: PyTorch}{29}{section.3.4}
\contentsline {subsection}{\numberline {3.4.1}Training on Google Colab}{30}{subsection.3.4.1}
\contentsline {subsubsection}{LSTM vs. GRU}{30}{section*.79}
\contentsline {subsection}{\numberline {3.4.2}Microsoft Azure Platform}{30}{subsection.3.4.2}
\contentsline {subsection}{\numberline {3.4.3}Training I: Using Tuda-De, M-AILABS, Common Voice }{30}{subsection.3.4.3}
\contentsline {subsubsection}{Speaker Independent Splitting}{30}{section*.81}
\contentsline {subsubsection}{Training Process and Results}{31}{section*.85}
\contentsline {subsection}{\numberline {3.4.4}Training II: Adding all data}{31}{subsection.3.4.4}
\contentsline {subsubsection}{problem of adding \ac {SWC}}{31}{section*.86}
\contentsline {subsubsection}{Speaker Independent Splitting}{31}{section*.87}
\contentsline {subsubsection}{Training Process and Results}{32}{section*.91}
\contentsline {subsection}{\numberline {3.4.5}Language Model Decoding}{32}{subsection.3.4.5}
\contentsline {subsection}{\numberline {3.4.6}Language Model Re-scoring}{34}{subsection.3.4.6}
\contentsline {subsection}{\numberline {3.4.7}Auto Correct With Transformer}{35}{subsection.3.4.7}
\contentsline {subsubsection}{Model Selection and Defining the Problem}{35}{section*.92}
\contentsline {subsubsection}{Training Data and Results}{35}{section*.94}
\contentsline {subsection}{\numberline {3.4.8}Comparing with the Hybrid Model}{36}{subsection.3.4.8}
\contentsline {section}{\numberline {3.5}Text Classifier Unit}{37}{section.3.5}
\contentsline {subsection}{\numberline {3.5.1}Why BERT?}{37}{subsection.3.5.1}
\contentsline {subsection}{\numberline {3.5.2}Implementation}{37}{subsection.3.5.2}
\contentsline {subsection}{\numberline {3.5.3}Pre-training Data}{37}{subsection.3.5.3}
\contentsline {subsubsection}{Pre-processing German Wikipedia}{38}{section*.96}
\contentsline {subsubsection}{Generating Vocab File}{38}{section*.97}
\contentsline {subsubsection}{Tokenization and Writing into TFRecord Format}{39}{section*.98}
\contentsline {subsection}{\numberline {3.5.4}Pre-Training Process}{39}{subsection.3.5.4}
\contentsline {subsection}{\numberline {3.5.5}Fine-Tuning Data}{40}{subsection.3.5.5}
\contentsline {subsubsection}{\ac {10KGNAD}}{40}{section*.99}
\contentsline {subsection}{\numberline {3.5.6}Fine-Tuning Process}{41}{subsection.3.5.6}
\contentsline {subsubsection}{Effect of Running Pre-Training on Fine-Tuning Data}{42}{section*.104}
\contentsline {chapter}{\numberline {4}Results}{43}{chapter.4}
\contentsline {section}{\numberline {4.1}Automatic Speech Recognition Unit: Deep Speech 2}{43}{section.4.1}
\contentsline {subsection}{\numberline {4.1.1}ASR Evaluation Metrics}{43}{subsection.4.1.1}
\contentsline {subsection}{\numberline {4.1.2}ASR Results}{44}{subsection.4.1.2}
\contentsline {subsubsection}{Results Summary}{44}{section*.106}
\contentsline {subsubsection}{Our Best Model}{44}{section*.107}
\contentsline {subsubsection}{Our Best Model vs Kaldi's Model}{44}{section*.108}
\contentsline {section}{\numberline {4.2}Text Classifier Unit: BERT}{44}{section.4.2}
\contentsline {subsection}{\numberline {4.2.1}BERT Evaluation Metrics}{44}{subsection.4.2.1}
\contentsline {subsection}{\numberline {4.2.2}BERT Results}{45}{subsection.4.2.2}
\contentsline {subsubsection}{Pre-Training Results}{45}{section*.110}
\contentsline {subsubsection}{Fine-Tuning Results}{45}{section*.112}
\contentsline {chapter}{\numberline {5}Conclusion and Future Work}{47}{chapter.5}
\contentsline {section}{\numberline {5.1}Conclusion}{47}{section.5.1}
\contentsline {section}{\numberline {5.2}Future Work}{47}{section.5.2}
\contentsline {chapter}{Appendix}{49}{section*.114}
\contentsline {chapter}{\numberline {A}Lists}{50}{appendix.A}
\contentsline {section}{List of Abbreviations}{50}{section*.115}
\contentsline {section}{List of Figures}{52}{appendix*.116}
\contentsline {chapter}{References}{55}{appendix*.117}
