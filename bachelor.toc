\contentsline {chapter}{Acknowledgments}{V}{chapter*.1}
\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}
\contentsline {section}{\numberline {1.1}Section Name}{1}{section.1.1}
\contentsline {section}{\numberline {1.2}Another Section}{1}{section.1.2}
\contentsline {chapter}{\numberline {2}Background}{3}{chapter.2}
\contentsline {section}{\numberline {2.1}Natural Language Processing}{3}{section.2.1}
\contentsline {section}{\numberline {2.2}Artificial Neural Networks}{3}{section.2.2}
\contentsline {subsection}{\numberline {2.2.1}Feed Forward Neural Networks}{4}{subsection.2.2.1}
\contentsline {subsubsection}{Sequence Models and the Problem with FNNs}{5}{section*.13}
\contentsline {subsection}{\numberline {2.2.2}Recurrent Neural Networks}{5}{subsection.2.2.2}
\contentsline {subsubsection}{Bidirectional RNNs}{6}{section*.17}
\contentsline {subsubsection}{Problems with RNNs}{7}{section*.20}
\contentsline {subsubsection}{Long Short-Term Memory}{7}{section*.21}
\contentsline {subsubsection}{Gated Recurrent Units}{8}{section*.24}
\contentsline {subsection}{\numberline {2.2.3}Convolution Neural Networks}{8}{subsection.2.2.3}
\contentsline {section}{\numberline {2.3}Speech Recognition}{8}{section.2.3}
\contentsline {subsection}{\numberline {2.3.1}End-to-End Speech Recognition}{8}{subsection.2.3.1}
\contentsline {subsection}{\numberline {2.3.2}Feature Extraction}{8}{subsection.2.3.2}
\contentsline {subsection}{\numberline {2.3.3}Building the Model}{8}{subsection.2.3.3}
\contentsline {subsection}{\numberline {2.3.4}Computing the Loss}{8}{subsection.2.3.4}
\contentsline {section}{\numberline {2.4}Text Analytics}{8}{section.2.4}
\contentsline {subsection}{\numberline {2.4.1}Encoder-Decoder Architecture}{9}{subsection.2.4.1}
\contentsline {subsection}{\numberline {2.4.2}Attention Mechanism}{9}{subsection.2.4.2}
\contentsline {subsubsection}{\MakeUppercase {i}. Encoder}{9}{section*.27}
\contentsline {subsubsection}{\MakeUppercase {ii}. Decoder}{10}{section*.29}
\contentsline {subsubsection}{\MakeUppercase {iii}. Additive Attention}{10}{section*.30}
\contentsline {subsection}{\numberline {2.4.3}The Transformer}{10}{subsection.2.4.3}
\contentsline {subsubsection}{\MakeUppercase {i}. Problem with Recurrence}{10}{section*.31}
\contentsline {subsubsection}{\MakeUppercase {ii}. Encoder}{11}{section*.32}
\contentsline {subsubsection}{\MakeUppercase {iii}. Decoder}{11}{section*.33}
\contentsline {subsubsection}{\MakeUppercase {iv}. Scaled Dot-Product Attention}{11}{section*.34}
\contentsline {subsubsection}{\MakeUppercase {v}. Positional Encodings}{11}{section*.35}
\contentsline {subsection}{\numberline {2.4.4}Bidirectional Encoder from Transformer (BERT)}{11}{subsection.2.4.4}
\contentsline {chapter}{\numberline {3}Conclusion}{13}{chapter.3}
\contentsline {chapter}{\numberline {4}Future Work}{15}{chapter.4}
\contentsline {chapter}{Appendix}{16}{section*.36}
\contentsline {chapter}{\numberline {A}Lists}{17}{appendix.A}
\contentsline {section}{List of Abbreviations}{17}{section*.37}
\contentsline {section}{List of Figures}{18}{appendix*.38}
\contentsline {chapter}{References}{20}{appendix*.39}
