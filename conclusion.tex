\chapter{Conclusion and Future Work}
\label{chap:concl}

\section{Conclusion}
\label{conc:s1}

The literature review demonstrated how neural networks have dominated the fields of speech recognition and contextual analysis resulting in many powerful models and architectures. In the introduction, we showed the need for efficient dispatching systems for better driver and passenger experience. 

We defined our problem to be to design and implement an automatic dispatching system, which given the speech input from the driver's side, issues the proper dispatcher action. Hence, automating the dispatcher side resulting in lower cost and reduced errors. 

Our proposed system was a pipeline consisting of two sub-systems, as illustrated in figure \ref{meth:fig1}. The first part is an \acf{ASR} Unit, with the objective of transcribing the speech signals communicated by the vehicle's driver. The transcribed audio is then fed into the second sub-system, which is a Text Classifier Unit. This unit is concerned with analyzing the context and the situation, and predicting the appropriate dispatcher action.


For the \ac{ASR} unit, we used an end-to-end system which is Deep Speech 2. We collected $\approx 700$ hours of transcribed German audio data. We collected also text data in order to implement language model decoding to achieve better \ac{WER}. We trained a German \ac{ASR} system able to transcribe speech with \ac{WER} of $\approx 15\%$ using a $5$-gram language model. 

As for the Text Classifier unit, we used Google's \ac{BERT} model and adapted it to German. We trained the model using the German Wikipedia on two pre-training tasks: \acf{MLM} and Next Sentence Prediction. Our system achieves \ac{MLM} accuracy of $63\%$ and next sentence accuracy of $98\%$. We assume our problem is a classification problem, and therefore fine-tune a classification layer on top of \ac{BERT}. Unfortunately, we found no datasets that could be used to train \ac{BERT} to classify dispatcher actions. Instead, we fine-tune on a general German classification dataset which is the \acf{10KGNAD}. We take that as a proof of concept that our pre-trained \ac{BERT} model could be fine-tuned as a classifier with promising results. Fine-tuning results in classification accuracy of $88\%$, and after running additional steps of pre-training on the dataset before fine-tuning, we obtain $90\%$ accuracy.


\section{Future Work}
\label{conc:s2}

Further research could be pursued on this topic in order for our system to be deployed:

\begin{enumerate}
	\item One obstacle we faced was our inability to obtain datasets for the driver's speech and the corresponding dispatcher action. Collection and processing of relevant dataset to fine-tune our text classifier unit on is a crucial step for brining our system into action. 
	\item Further experimentation could be explored with \ac{BERT}, such as training with different vocab sizes, longer sequence lengths, and different hyper-parameters and training steps.
	\item The broken re-scoring mechanism we implemented for the \ac{ASR} unit could be investigated and amended.
	\item The trained Transformer intended for \ac{ASR} auto-correction could be tested on our test set in order to compare it with our best model.

\end{enumerate}


