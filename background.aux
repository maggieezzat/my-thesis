\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{hopfield1982neural}
\citation{jordan1986serial}
\citation{elman1990finding}
\citation{A critical review of recurrent neural networks for sequence learning}
\citation{A critical review of recurrent neural networks for sequence learning}
\citation{rumelhart1985learning}
\citation{werbos1988generalization}
\citation{bishop1995neural}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Literature Review}{3}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:background}{{2}{3}{Literature Review}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Natural Language Processing}{3}{section.2.1}}
\newlabel{bg:s1}{{2.1}{3}{Natural Language Processing}{section.2.1}{}}
\AC@undonewlabel{acro:NLP}
\newlabel{acro:NLP}{{2.1}{3}{Natural Language Processing}{section*.5}{}}
\acronymused{NLP}
\AC@undonewlabel{acro:NLG}
\newlabel{acro:NLG}{{2.1}{3}{Natural Language Processing}{section*.6}{}}
\acronymused{NLG}
\AC@undonewlabel{acro:NLU}
\newlabel{acro:NLU}{{2.1}{3}{Natural Language Processing}{section*.7}{}}
\acronymused{NLU}
\acronymused{NLG}
\acronymused{NLU}
\acronymused{NLP}
\acronymused{ASR}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Artificial Neural Networks}{3}{section.2.2}}
\newlabel{bg:s2}{{2.2}{3}{Artificial Neural Networks}{section.2.2}{}}
\AC@undonewlabel{acro:ANN}
\newlabel{acro:ANN}{{2.2}{3}{Artificial Neural Networks}{section*.8}{}}
\acronymused{ANN}
\acronymused{ANN}
\acronymused{ANN}
\acronymused{ANN}
\citation{rumelhart1985learning}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces A neuron represented as a circle and the weighted edges as arrows. The activation function is a function of the sum of the weighted edges. \cite  {A critical review of recurrent neural networks for sequence learning}\relax }}{4}{figure.caption.9}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{Fig:1}{{2.1}{4}{A neuron represented as a circle and the weighted edges as arrows. The activation function is a function of the sum of the weighted edges. \cite {A critical review of recurrent neural networks for sequence learning}\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Feed Forward Neural Networks}{4}{subsection.2.2.1}}
\newlabel{bg:sub1}{{2.2.1}{4}{Feed Forward Neural Networks}{subsection.2.2.1}{}}
\AC@undonewlabel{acro:FNN}
\newlabel{acro:FNN}{{2.2.1}{4}{Feed Forward Neural Networks}{section*.10}{}}
\acronymused{FNN}
\AC@undonewlabel{acro:MLP}
\newlabel{acro:MLP}{{2.2.1}{4}{Feed Forward Neural Networks}{section*.11}{}}
\acronymused{MLP}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces A simple Feed Forward Neural Network consisting of an input layer, one hidden layer, and an output layer\relax }}{4}{figure.caption.12}}
\newlabel{Fig:2}{{2.2}{4}{A simple Feed Forward Neural Network consisting of an input layer, one hidden layer, and an output layer\relax }{figure.caption.12}{}}
\acronymused{FNN}
\citation{werbos1990backpropagation}
\acronymused{FNN}
\@writefile{toc}{\contentsline {subsubsection}{Sequence Models and the Problem with \ac {FNN}s}{5}{section*.13}}
\newlabel{bg:subsub1}{{2.2.1}{5}{Sequence Models and the Problem with \ac {FNN}s}{section*.13}{}}
\acronymused{FNN}
\acronymused{FNN}
\acronymused{FNN}
\acronymused{FNN}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Recurrent Neural Networks}{5}{subsection.2.2.2}}
\newlabel{bg:sub2}{{2.2.2}{5}{Recurrent Neural Networks}{subsection.2.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces A Recurrent Neural Network\relax }}{5}{figure.caption.14}}
\newlabel{Fig:4}{{2.3}{5}{A Recurrent Neural Network\relax }{figure.caption.14}{}}
\acronymused{ANN}
\AC@undonewlabel{acro:RNN}
\newlabel{acro:RNN}{{2.2.2}{5}{Recurrent Neural Networks}{section*.15}{}}
\acronymused{RNN}
\acronymused{RNN}
\citation{schuster1997bidirectional}
\citation{hochreiter1991untersuchungen}
\citation{hochreiter2001gradient}
\citation{bengio1994learning}
\citation{hochreiter1997long}
\acronymused{ANN}
\AC@undonewlabel{acro:BPTT}
\newlabel{acro:BPTT}{{2.2.2}{6}{Recurrent Neural Networks}{section*.16}{}}
\acronymused{BPTT}
\newlabel{eq:1}{{2.1}{6}{Recurrent Neural Networks}{equation.2.2.1}{}}
\newlabel{eq:2}{{2.2}{6}{Recurrent Neural Networks}{equation.2.2.2}{}}
\acronymused{RNN}
\@writefile{toc}{\contentsline {subsubsection}{Bidirectional \ac {RNN}s}{6}{section*.17}}
\newlabel{bg:subsub3}{{2.2.2}{6}{Bidirectional \ac {RNN}s}{section*.17}{}}
\acronymused{RNN}
\AC@undonewlabel{acro:BRNN}
\newlabel{acro:BRNN}{{2.2.2}{6}{Bidirectional \ac {RNN}s}{section*.18}{}}
\acronymused{BRNN}
\acronymused{BRNN}
\acronymused{BRNN}
\acronymused{BRNN}
\acronymused{BRNN}
\acronymused{BRNN}
\newlabel{eq:4}{{2.3}{6}{Bidirectional \ac {RNN}s}{equation.2.2.3}{}}
\newlabel{eq:5}{{2.4}{6}{Bidirectional \ac {RNN}s}{equation.2.2.4}{}}
\newlabel{eq:6}{{2.5}{6}{Bidirectional \ac {RNN}s}{equation.2.2.5}{}}
\acronymused{RNN}
\@writefile{toc}{\contentsline {subsubsection}{Problems with \ac {RNN}s}{6}{section*.20}}
\newlabel{bg:subsub4}{{2.2.2}{6}{Problems with \ac {RNN}s}{section*.20}{}}
\@writefile{toc}{\contentsline {subsubsection}{Long Short-Term Memory}{6}{section*.21}}
\newlabel{bg:subsub5}{{2.2.2}{6}{Long Short-Term Memory}{section*.21}{}}
\AC@undonewlabel{acro:LSTM}
\newlabel{acro:LSTM}{{2.2.2}{6}{Long Short-Term Memory}{section*.22}{}}
\acronymused{LSTM}
\acronymused{RNN}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces A Bidirectional Recurrent Neural Network\relax }}{7}{figure.caption.19}}
\newlabel{Fig:5}{{2.4}{7}{A Bidirectional Recurrent Neural Network\relax }{figure.caption.19}{}}
\newlabel{eq:10}{{2.6}{7}{Long Short-Term Memory}{equation.2.2.6}{}}
\newlabel{eq:7}{{2.7}{7}{Long Short-Term Memory}{equation.2.2.7}{}}
\newlabel{eq:8}{{2.8}{7}{Long Short-Term Memory}{equation.2.2.8}{}}
\newlabel{eq:9}{{2.9}{7}{Long Short-Term Memory}{equation.2.2.9}{}}
\newlabel{eq:11}{{2.10}{7}{Long Short-Term Memory}{equation.2.2.10}{}}
\newlabel{eq:12}{{2.11}{7}{Long Short-Term Memory}{equation.2.2.11}{}}
\acronymused{LSTM}
\@writefile{toc}{\contentsline {subsubsection}{Problems with \ac {LSTM}}{7}{section*.24}}
\newlabel{conc:subsub7}{{2.2.2}{7}{Problems with \ac {LSTM}}{section*.24}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Convolution Neural Networks}{7}{subsection.2.2.3}}
\newlabel{bg:sub3}{{2.2.3}{7}{Convolution Neural Networks}{subsection.2.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces An \ac {LSTM} Cell\relax }}{8}{figure.caption.23}}
\acronymused{LSTM}
\newlabel{Fig:6}{{2.5}{8}{An \ac {LSTM} Cell\relax }{figure.caption.23}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Speech Recognition}{8}{section.2.3}}
\newlabel{bg:s3}{{2.3}{8}{Speech Recognition}{section.2.3}{}}
\AC@undonewlabel{acro:HMM}
\newlabel{acro:HMM}{{2.3}{8}{Speech Recognition}{section*.25}{}}
\acronymused{HMM}
\acronymused{ASR}
\acronymused{HMM}
\acronymused{ASR}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}HMM-Based \ac {ASR} Systems}{8}{subsection.2.3.1}}
\newlabel{bg:sub4}{{2.3.1}{8}{HMM-Based \ac {ASR} Systems}{subsection.2.3.1}{}}
\acronymused{ASR}
\acronymused{ASR}
\newlabel{eq:24}{{2.12}{8}{HMM-Based \ac {ASR} Systems}{equation.2.3.12}{}}
\acronymused{ASR}
\acronymused{ASR}
\acronymused{HMM}
\@writefile{toc}{\contentsline {subsubsection}{Feature Extraction}{9}{section*.26}}
\newlabel{bg:sub5}{{2.3.1}{9}{Feature Extraction}{section*.26}{}}
\acronymused{ASR}
\AC@undonewlabel{acro:MFCC}
\newlabel{acro:MFCC}{{2.3.1}{9}{Feature Extraction}{section*.27}{}}
\acronymused{MFCC}
\AC@undonewlabel{acro:FFT}
\newlabel{acro:FFT}{{2.3.1}{9}{Feature Extraction}{section*.28}{}}
\acronymused{FFT}
\AC@undonewlabel{acro:DCT}
\newlabel{acro:DCT}{{2.3.1}{9}{Feature Extraction}{section*.29}{}}
\acronymused{DCT}
\acronymused{DCT}
\acronymused{DCT}
\acronymused{ASR}
\@writefile{toc}{\contentsline {subsubsection}{Acoustic Model}{9}{section*.30}}
\newlabel{bg:sub5}{{2.3.1}{9}{Acoustic Model}{section*.30}{}}
\@writefile{toc}{\contentsline {subsubsection}{Language Model}{9}{section*.31}}
\newlabel{bg:sub5}{{2.3.1}{9}{Language Model}{section*.31}{}}
\citation{graves2006connectionist}
\@writefile{toc}{\contentsline {subsubsection}{Decoding}{10}{section*.32}}
\newlabel{bg:sub5}{{2.3.1}{10}{Decoding}{section*.32}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Hybrid Systems}{10}{subsection.2.3.2}}
\newlabel{bg:sub7}{{2.3.2}{10}{Hybrid Systems}{subsection.2.3.2}{}}
\acronymused{RNN}
\acronymused{RNN}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}End-to-End Systems}{10}{subsection.2.3.3}}
\newlabel{bg:sub7}{{2.3.3}{10}{End-to-End Systems}{subsection.2.3.3}{}}
\acronymused{RNN}
\AC@undonewlabel{acro:CTC}
\newlabel{acro:CTC}{{2.3.3}{10}{End-to-End Systems}{section*.33}{}}
\acronymused{CTC}
\acronymused{RNN}
\acronymused{BPTT}
\acronymused{RNN}
\newlabel{eq:21}{{2.13}{10}{End-to-End Systems}{equation.2.3.13}{}}
\citation{cho2014learning}
\citation{sutskever2014sequence}
\citation{sutskever2014sequence}
\citation{cho2014learning}
\newlabel{eq:22}{{2.14}{11}{End-to-End Systems}{equation.2.3.14}{}}
\acronymused{CTC}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Text Analytics}{11}{section.2.4}}
\newlabel{bg:s4}{{2.4}{11}{Text Analytics}{section.2.4}{}}
\acronymused{RNN}
\acronymused{RNN}
\acronymused{RNN}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Encoder-Decoder Architecture}{11}{subsection.2.4.1}}
\newlabel{bg:sub8}{{2.4.1}{11}{Encoder-Decoder Architecture}{subsection.2.4.1}{}}
\AC@undonewlabel{acro:RNNs}
\newlabel{acro:RNNs}{{2.4.1}{11}{Encoder-Decoder Architecture}{section*.34}{}}
\acronymused{RNNs}
\citation{cho2014properties}
\citation{bahdanau2014neural}
\citation{bahdanau2014neural}
\citation{bahdanau2014neural}
\acronymused{RNN}
\acronymused{LSTM}
\acronymused{LSTM}
\newlabel{eq:13}{{2.15}{12}{Encoder-Decoder Architecture}{equation.2.4.15}{}}
\acronymused{RNN}
\newlabel{eq:14}{{2.16}{12}{Encoder-Decoder Architecture}{equation.2.4.16}{}}
\newlabel{eq:15}{{2.17}{12}{Encoder-Decoder Architecture}{equation.2.4.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Attention Mechanism}{12}{subsection.2.4.2}}
\newlabel{bg:sub9}{{2.4.2}{12}{Attention Mechanism}{subsection.2.4.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\MakeUppercase  {i}. Encoder}{12}{section*.35}}
\newlabel{bg:subsub9}{{2.4.2}{12}{\RomanNumeralCaps {1}. Encoder}{section*.35}{}}
\acronymused{BRNN}
\@writefile{toc}{\contentsline {subsubsection}{\MakeUppercase  {ii}. Decoder}{13}{section*.36}}
\newlabel{bg:subsub10}{{2.4.2}{13}{\RomanNumeralCaps {2}. Decoder}{section*.36}{}}
\acronymused{RNN}
\newlabel{eq:16}{{2.18}{13}{\RomanNumeralCaps {2}. Decoder}{equation.2.4.18}{}}
\newlabel{eq:17}{{2.19}{13}{\RomanNumeralCaps {2}. Decoder}{equation.2.4.19}{}}
\newlabel{eq:18}{{2.20}{13}{\RomanNumeralCaps {2}. Decoder}{equation.2.4.20}{}}
\newlabel{eq:19}{{2.21}{13}{\RomanNumeralCaps {2}. Decoder}{equation.2.4.21}{}}
\newlabel{eq:20}{{2.22}{13}{\RomanNumeralCaps {2}. Decoder}{equation.2.4.22}{}}
\@writefile{toc}{\contentsline {subsubsection}{\MakeUppercase  {iii}. Additive Attention}{13}{section*.37}}
\newlabel{bg:subsub100}{{2.4.2}{13}{\RomanNumeralCaps {3}. Additive Attention}{section*.37}{}}
\acronymused{RNN}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}The Transformer}{13}{subsection.2.4.3}}
\newlabel{bg:sub10}{{2.4.3}{13}{The Transformer}{subsection.2.4.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\MakeUppercase  {i}. Problem with Recurrence}{13}{section*.38}}
\newlabel{bg:subsub11}{{2.4.3}{13}{\RomanNumeralCaps {1}. Problem with Recurrence}{section*.38}{}}
\acronymused{RNNs}
\citation{he2016deep}
\citation{ba2016layer}
\@writefile{toc}{\contentsline {subsubsection}{\MakeUppercase  {ii}. Encoder}{14}{section*.39}}
\newlabel{bg:subsub12}{{2.4.3}{14}{\RomanNumeralCaps {2}. Encoder}{section*.39}{}}
\@writefile{toc}{\contentsline {subsubsection}{\MakeUppercase  {iii}. Decoder}{14}{section*.40}}
\newlabel{bg:subsub13}{{2.4.3}{14}{\RomanNumeralCaps {3}. Decoder}{section*.40}{}}
\@writefile{toc}{\contentsline {subsubsection}{\MakeUppercase  {iv}. Scaled Dot-Product Attention}{14}{section*.41}}
\newlabel{bg:subsub14}{{2.4.3}{14}{\RomanNumeralCaps {4}. Scaled Dot-Product Attention}{section*.41}{}}
\@writefile{toc}{\contentsline {subsubsection}{\MakeUppercase  {v}. Positional Encodings}{14}{section*.42}}
\newlabel{bg:subsub15}{{2.4.3}{14}{\RomanNumeralCaps {5}. Positional Encodings}{section*.42}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.4}Bidirectional Encoder from Transformer (BERT)}{14}{subsection.2.4.4}}
\newlabel{bg:sub11}{{2.4.4}{14}{Bidirectional Encoder from Transformer (BERT)}{subsection.2.4.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Word Embeddings}{14}{section*.43}}
\@writefile{toc}{\contentsline {subsubsection}{Transfer Learning}{14}{section*.44}}
\@writefile{toc}{\contentsline {paragraph}{Feature Extraction}{14}{section*.45}}
\@writefile{toc}{\contentsline {paragraph}{Fine Tuning}{14}{section*.46}}
\@writefile{toc}{\contentsline {subsubsection}{Bidirectional vs uni-directional}{14}{section*.47}}
\@writefile{toc}{\contentsline {subsubsection}{PreTraining Tasks}{14}{section*.48}}
\@setckpt{background}{
\setcounter{page}{15}
\setcounter{equation}{22}
\setcounter{enumi}{2}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{4}
\setcounter{subsection}{4}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{5}
\setcounter{table}{0}
\setcounter{parentequation}{0}
\setcounter{float@type}{8}
\setcounter{ContinuedFloat}{0}
\setcounter{KVtest}{0}
\setcounter{subfigure}{0}
\setcounter{subfigure@save}{0}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{subtable@save}{0}
\setcounter{lotdepth}{1}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{Item}{4}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{17}
\setcounter{ALC@unique}{0}
\setcounter{ALC@line}{0}
\setcounter{ALC@rem}{0}
\setcounter{ALC@depth}{0}
\setcounter{algorithm}{0}
\setcounter{section@level}{3}
}
