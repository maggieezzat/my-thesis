\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{hopfield1982neural}
\citation{jordan1986serial}
\citation{elman1990finding}
\citation{A critical review of recurrent neural networks for sequence learning}
\citation{A critical review of recurrent neural networks for sequence learning}
\citation{rumelhart1985learning}
\citation{werbos1988generalization}
\citation{bishop1995neural}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Literature Review}{3}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:bg}{{2}{3}{Literature Review}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Natural Language Processing}{3}{section.2.1}}
\newlabel{bg:s1}{{2.1}{3}{Natural Language Processing}{section.2.1}{}}
\acronymused{NLP}
\AC@undonewlabel{acro:NLG}
\newlabel{acro:NLG}{{2.1}{3}{Natural Language Processing}{section*.7}{}}
\acronymused{NLG}
\AC@undonewlabel{acro:NLU}
\newlabel{acro:NLU}{{2.1}{3}{Natural Language Processing}{section*.8}{}}
\acronymused{NLU}
\acronymused{NLG}
\acronymused{NLU}
\acronymused{NLP}
\acronymused{ASR}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Artificial Neural Networks}{3}{section.2.2}}
\newlabel{bg:s2}{{2.2}{3}{Artificial Neural Networks}{section.2.2}{}}
\AC@undonewlabel{acro:ANN}
\newlabel{acro:ANN}{{2.2}{3}{Artificial Neural Networks}{section*.9}{}}
\acronymused{ANN}
\acronymused{ANN}
\acronymused{ANN}
\acronymused{ANN}
\citation{rumelhart1985learning}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces A neuron represented as a circle and the weighted edges as arrows. The activation function is a function of the sum of the weighted edges. \cite  {A critical review of recurrent neural networks for sequence learning}\relax }}{4}{figure.caption.10}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{Fig:1}{{2.1}{4}{A neuron represented as a circle and the weighted edges as arrows. The activation function is a function of the sum of the weighted edges. \cite {A critical review of recurrent neural networks for sequence learning}\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Feed Forward Neural Networks}{4}{subsection.2.2.1}}
\newlabel{bg:s2_sub1}{{2.2.1}{4}{Feed Forward Neural Networks}{subsection.2.2.1}{}}
\AC@undonewlabel{acro:FNN}
\newlabel{acro:FNN}{{2.2.1}{4}{Feed Forward Neural Networks}{section*.11}{}}
\acronymused{FNN}
\AC@undonewlabel{acro:MLP}
\newlabel{acro:MLP}{{2.2.1}{4}{Feed Forward Neural Networks}{section*.12}{}}
\acronymused{MLP}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces A simple Feed Forward Neural Network consisting of an input layer, one hidden layer, and an output layer\relax }}{4}{figure.caption.13}}
\newlabel{Fig:2}{{2.2}{4}{A simple Feed Forward Neural Network consisting of an input layer, one hidden layer, and an output layer\relax }{figure.caption.13}{}}
\acronymused{FNN}
\citation{werbos1990backpropagation}
\acronymused{FNN}
\@writefile{toc}{\contentsline {subsubsection}{Sequence Models and the Problem with \ac {FNN}s}{5}{section*.14}}
\newlabel{bg:s2_sub1_subsub1}{{2.2.1}{5}{Sequence Models and the Problem with \ac {FNN}s}{section*.14}{}}
\acronymused{FNN}
\acronymused{FNN}
\acronymused{FNN}
\acronymused{FNN}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Recurrent Neural Networks}{5}{subsection.2.2.2}}
\newlabel{bg:s2_sub2}{{2.2.2}{5}{Recurrent Neural Networks}{subsection.2.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces A Recurrent Neural Network\relax }}{5}{figure.caption.15}}
\newlabel{Fig:4}{{2.3}{5}{A Recurrent Neural Network\relax }{figure.caption.15}{}}
\acronymused{ANN}
\AC@undonewlabel{acro:RNN}
\newlabel{acro:RNN}{{2.2.2}{5}{Recurrent Neural Networks}{section*.16}{}}
\acronymused{RNN}
\acronymused{RNN}
\citation{schuster1997bidirectional}
\citation{bengio1994learning}
\citation{hochreiter2001gradient}
\citation{hochreiter1997long}
\acronymused{ANN}
\AC@undonewlabel{acro:BPTT}
\newlabel{acro:BPTT}{{2.2.2}{6}{Recurrent Neural Networks}{section*.17}{}}
\acronymused{BPTT}
\newlabel{eq:1}{{2.1}{6}{Recurrent Neural Networks}{equation.2.2.1}{}}
\newlabel{eq:2}{{2.2}{6}{Recurrent Neural Networks}{equation.2.2.2}{}}
\acronymused{RNN}
\@writefile{toc}{\contentsline {subsubsection}{Bidirectional \ac {RNN}s}{6}{section*.18}}
\newlabel{bg:s2_sub2_subsub1}{{2.2.2}{6}{Bidirectional \ac {RNN}s}{section*.18}{}}
\acronymused{RNN}
\AC@undonewlabel{acro:BRNN}
\newlabel{acro:BRNN}{{2.2.2}{6}{Bidirectional \ac {RNN}s}{section*.19}{}}
\acronymused{BRNN}
\acronymused{BRNN}
\acronymused{BRNN}
\acronymused{BRNN}
\acronymused{BRNN}
\acronymused{BRNN}
\newlabel{eq:4}{{2.3}{6}{Bidirectional \ac {RNN}s}{equation.2.2.3}{}}
\newlabel{eq:5}{{2.4}{6}{Bidirectional \ac {RNN}s}{equation.2.2.4}{}}
\newlabel{eq:6}{{2.5}{6}{Bidirectional \ac {RNN}s}{equation.2.2.5}{}}
\acronymused{RNN}
\@writefile{toc}{\contentsline {subsubsection}{Problems with \ac {RNN}s: Vanishing and Exploding Gradients}{6}{section*.21}}
\newlabel{bg:s2_sub2_subsub2}{{2.2.2}{6}{Problems with \ac {RNN}s: Vanishing and Exploding Gradients}{section*.21}{}}
\acronymused{RNN}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces A Bidirectional Recurrent Neural Network\relax }}{7}{figure.caption.20}}
\newlabel{Fig:5}{{2.4}{7}{A Bidirectional Recurrent Neural Network\relax }{figure.caption.20}{}}
\@writefile{toc}{\contentsline {subsubsection}{Long Short-Term Memory}{7}{section*.22}}
\newlabel{bg:s2_sub2_subsub3}{{2.2.2}{7}{Long Short-Term Memory}{section*.22}{}}
\AC@undonewlabel{acro:LSTM}
\newlabel{acro:LSTM}{{2.2.2}{7}{Long Short-Term Memory}{section*.23}{}}
\acronymused{LSTM}
\acronymused{RNN}
\acronymused{LSTM}
\newlabel{eq:100}{{2.6}{7}{Long Short-Term Memory}{equation.2.2.6}{}}
\newlabel{eq:101}{{2.7}{7}{Long Short-Term Memory}{equation.2.2.7}{}}
\newlabel{eq:102}{{2.8}{7}{Long Short-Term Memory}{equation.2.2.8}{}}
\newlabel{eq:103}{{2.9}{7}{Long Short-Term Memory}{equation.2.2.9}{}}
\newlabel{eq:104}{{2.10}{7}{Long Short-Term Memory}{equation.2.2.10}{}}
\newlabel{eq:105}{{2.11}{7}{Long Short-Term Memory}{equation.2.2.11}{}}
\acronymused{LSTM}
\acronymused{LSTM}
\AC@undonewlabel{acro:GRU}
\newlabel{acro:GRU}{{2.2.2}{7}{Long Short-Term Memory}{section*.24}{}}
\acronymused{GRU}
\acronymused{LSTM}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Automatic Speech Recognition}{7}{section.2.3}}
\newlabel{bg:s3}{{2.3}{7}{Automatic Speech Recognition}{section.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces An \ac {LSTM} Cell\relax }}{8}{figure.caption.25}}
\acronymused{LSTM}
\newlabel{Fig:6}{{2.5}{8}{An \ac {LSTM} Cell\relax }{figure.caption.25}{}}
\AC@undonewlabel{acro:HMM}
\newlabel{acro:HMM}{{2.3}{8}{Automatic Speech Recognition}{section*.26}{}}
\acronymused{HMM}
\acronymused{ASR}
\acronymused{HMM}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}HMM-Based ASR Systems}{8}{subsection.2.3.1}}
\newlabel{bg:s3_sub1}{{2.3.1}{8}{HMM-Based ASR Systems}{subsection.2.3.1}{}}
\acronymused{ASR}
\acronymused{ASR}
\newlabel{eq:24}{{2.13}{8}{HMM-Based ASR Systems}{equation.2.3.13}{}}
\acronymused{ASR}
\acronymused{ASR}
\acronymused{HMM}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces An ASR System\relax }}{9}{figure.caption.27}}
\newlabel{Fig:7}{{2.6}{9}{An ASR System\relax }{figure.caption.27}{}}
\@writefile{toc}{\contentsline {subsubsection}{Feature Extraction}{9}{section*.28}}
\newlabel{bg:s3_sub1_subsub1}{{2.3.1}{9}{Feature Extraction}{section*.28}{}}
\acronymused{ASR}
\AC@undonewlabel{acro:MFCC}
\newlabel{acro:MFCC}{{2.3.1}{9}{Feature Extraction}{section*.30}{}}
\acronymused{MFCC}
\AC@undonewlabel{acro:FFT}
\newlabel{acro:FFT}{{2.3.1}{9}{Feature Extraction}{section*.31}{}}
\acronymused{FFT}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces MFCC\relax }}{10}{figure.caption.29}}
\newlabel{Fig:7}{{2.7}{10}{MFCC\relax }{figure.caption.29}{}}
\AC@undonewlabel{acro:DCT}
\newlabel{acro:DCT}{{2.3.1}{10}{Feature Extraction}{section*.32}{}}
\acronymused{DCT}
\acronymused{DCT}
\acronymused{DCT}
\acronymused{ASR}
\@writefile{toc}{\contentsline {subsubsection}{Acoustic Model}{10}{section*.33}}
\newlabel{bg:s3_sub1_subsub2}{{2.3.1}{10}{Acoustic Model}{section*.33}{}}
\acronymused{HMM}
\acronymused{HMM}
\acronymused{HMM}
\AC@undonewlabel{acro:FSM}
\newlabel{acro:FSM}{{2.3.1}{10}{Acoustic Model}{section*.34}{}}
\acronymused{FSM}
\acronymused{HMM}
\acronymused{HMM}
\acronymused{HMM}
\citation{keselj2009speech}
\citation{rabiner1989tutorial}
\citation{keselj2009speech}
\citation{baum1972inequality}
\acronymused{HMM}
\acronymused{HMM}
\@writefile{toc}{\contentsline {subsubsection}{Language Model}{11}{section*.35}}
\newlabel{bg:s3_sub1_subsub3}{{2.3.1}{11}{Language Model}{section*.35}{}}
\citation{graves2006connectionist}
\acronymused{RNN}
\acronymused{RNN}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}End-to-End Systems}{12}{subsection.2.3.2}}
\newlabel{bg:s3_sub2}{{2.3.2}{12}{End-to-End Systems}{subsection.2.3.2}{}}
\acronymused{RNN}
\AC@undonewlabel{acro:CTC}
\newlabel{acro:CTC}{{2.3.2}{12}{End-to-End Systems}{section*.36}{}}
\acronymused{CTC}
\acronymused{RNN}
\acronymused{BPTT}
\acronymused{RNN}
\newlabel{eq:21}{{2.15}{12}{End-to-End Systems}{equation.2.3.15}{}}
\newlabel{eq:22}{{2.16}{12}{End-to-End Systems}{equation.2.3.16}{}}
\citation{amodei2016deep}
\citation{amodei2016deep}
\citation{amodei2016deep}
\citation{ioffe2015batch}
\acronymused{CTC}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Deep Speech 2: End-to-End Speech Recognition Model}{13}{subsection.2.3.3}}
\newlabel{bg:s3_sub3}{{2.3.3}{13}{Deep Speech 2: End-to-End Speech Recognition Model}{subsection.2.3.3}{}}
\acronymused{ASR}
\acronymused{ASR}
\acronymused{ANN}
\acronymused{RNN}
\AC@undonewlabel{acro:CNN}
\newlabel{acro:CNN}{{2.3.3}{13}{Deep Speech 2: End-to-End Speech Recognition Model}{section*.38}{}}
\acronymused{CNN}
\acronymused{CTC}
\acronymused{RNN}
\acronymused{CNN}
\@writefile{toc}{\contentsline {subsubsection}{Sorta Grad}{13}{section*.39}}
\newlabel{bg:s3_sub3_subsub1}{{2.3.3}{13}{Sorta Grad}{section*.39}{}}
\acronymused{CTC}
\acronymused{CTC}
\citation{devlin2018bert}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Deep Speech 2 Model Architecture \cite  {amodei2016deep}\relax }}{14}{figure.caption.37}}
\newlabel{Fig:12}{{2.8}{14}{Deep Speech 2 Model Architecture \cite {amodei2016deep}\relax }{figure.caption.37}{}}
\@writefile{toc}{\contentsline {subsubsection}{Batch Normalization}{14}{section*.40}}
\newlabel{bg:s3_sub3_subsub2}{{2.3.3}{14}{Batch Normalization}{section*.40}{}}
\acronymused{RNN}
\newlabel{eq:30}{{2.17}{14}{Batch Normalization}{equation.2.3.17}{}}
\@writefile{toc}{\contentsline {subsubsection}{Look-ahead Convolution}{14}{section*.41}}
\newlabel{bg:s3_sub3_subsub3}{{2.3.3}{14}{Look-ahead Convolution}{section*.41}{}}
\acronymused{RNN}
\acronymused{RNN}
\acronymused{RNN}
\citation{cho2014learning}
\citation{sutskever2014sequence}
\citation{sutskever2014sequence}
\citation{cho2014learning}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Look-ahead Convolution architecture with future context size of 2\relax }}{15}{figure.caption.42}}
\newlabel{fig:50}{{2.9}{15}{Look-ahead Convolution architecture with future context size of 2\relax }{figure.caption.42}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Text Analysis}{15}{section.2.4}}
\newlabel{bg:s4}{{2.4}{15}{Text Analysis}{section.2.4}{}}
\acronymused{NLP}
\AC@undonewlabel{acro:BERT}
\newlabel{acro:BERT}{{2.4}{15}{Text Analysis}{section*.43}{}}
\acronymused{BERT}
\acronymused{BERT}
\acronymused{BERT}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Encoder-Decoder Architecture}{15}{subsection.2.4.1}}
\newlabel{bg:s4_sub1}{{2.4.1}{15}{Encoder-Decoder Architecture}{subsection.2.4.1}{}}
\acronymused{RNN}
\citation{cho2014properties}
\citation{bahdanau2014neural}
\citation{bahdanau2014neural}
\citation{bahdanau2014neural}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces An Encoder-Decoder System. The encoder encodes a variable-length sequence into a fixed-length vector $\mathbf  {c}$. The decoder uses the summary vector $\mathbf  {c}$ along with the previously generated predicted symbol from the previous time step $y_{t-1}$ and the current hidden state $\mathbf  {s_t}$ to generate an output $y_t$ \relax }}{16}{figure.caption.44}}
\newlabel{Fig:7}{{2.10}{16}{An Encoder-Decoder System. The encoder encodes a variable-length sequence into a fixed-length vector $\mathbf {c}$. The decoder uses the summary vector $\mathbf {c}$ along with the previously generated predicted symbol from the previous time step $y_{t-1}$ and the current hidden state $\mathbf {s_t}$ to generate an output $y_t$ \relax }{figure.caption.44}{}}
\acronymused{RNN}
\acronymused{LSTM}
\acronymused{LSTM}
\newlabel{eq:13}{{2.18}{16}{Encoder-Decoder Architecture}{equation.2.4.18}{}}
\acronymused{RNN}
\newlabel{eq:14}{{2.19}{16}{Encoder-Decoder Architecture}{equation.2.4.19}{}}
\newlabel{eq:15}{{2.20}{16}{Encoder-Decoder Architecture}{equation.2.4.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Attention Mechanism}{16}{subsection.2.4.2}}
\newlabel{bg:s4_sub2}{{2.4.2}{16}{Attention Mechanism}{subsection.2.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces The modified encoder-decoder system implementing the attention mechanism. The output $y_i$ has a corresponding context vector $\mathbf  {c_i}$ which is a summation of the weighted annotations $h_j$. The most relevant annotations in the input sentence have the largest weights $\alpha _{ij}$, while the least relevant annotations have the smallest weights. \relax }}{17}{figure.caption.45}}
\newlabel{Fig:8}{{2.11}{17}{The modified encoder-decoder system implementing the attention mechanism. The output $y_i$ has a corresponding context vector $\mathbf {c_i}$ which is a summation of the weighted annotations $h_j$. The most relevant annotations in the input sentence have the largest weights $\alpha _{ij}$, while the least relevant annotations have the smallest weights. \relax }{figure.caption.45}{}}
\@writefile{toc}{\contentsline {subsubsection}{\MakeUppercase  {i}. Encoder}{17}{section*.46}}
\newlabel{bg:s4_sub2_subsub1}{{2.4.2}{17}{\RomanNumeralCaps {1}. Encoder}{section*.46}{}}
\acronymused{BRNN}
\@writefile{toc}{\contentsline {subsubsection}{\MakeUppercase  {ii}. Decoder}{17}{section*.47}}
\newlabel{bg:s4_sub2_subsub2}{{2.4.2}{17}{\RomanNumeralCaps {2}. Decoder}{section*.47}{}}
\acronymused{RNN}
\newlabel{eq:16}{{2.21}{17}{\RomanNumeralCaps {2}. Decoder}{equation.2.4.21}{}}
\citation{vaswani2017attention}
\citation{he2016deep}
\citation{ba2016layer}
\newlabel{eq:17}{{2.22}{18}{\RomanNumeralCaps {2}. Decoder}{equation.2.4.22}{}}
\newlabel{eq:18}{{2.23}{18}{\RomanNumeralCaps {2}. Decoder}{equation.2.4.23}{}}
\newlabel{eq:19}{{2.24}{18}{\RomanNumeralCaps {2}. Decoder}{equation.2.4.24}{}}
\acronymused{FNN}
\acronymused{RNN}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}The Transformer}{18}{subsection.2.4.3}}
\newlabel{bg:s4_sub3}{{2.4.3}{18}{The Transformer}{subsection.2.4.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces The Transformer architecture \relax }}{18}{figure.caption.48}}
\newlabel{Fig:9}{{2.12}{18}{The Transformer architecture \relax }{figure.caption.48}{}}
\@writefile{toc}{\contentsline {subsubsection}{Problem with Recurrence}{18}{section*.49}}
\newlabel{bg:s4_sub3_subsub1}{{2.4.3}{18}{Problem with Recurrence}{section*.49}{}}
\acronymused{RNN}
\citation{gehring2017convolutional}
\citation{vaswani2017attention}
\citation{bahdanau2014neural}
\@writefile{toc}{\contentsline {subsubsection}{I. Encoder}{19}{section*.50}}
\newlabel{bg:s4_sub3_subsub2}{{2.4.3}{19}{I. Encoder}{section*.50}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces Self Attention Weights \relax }}{19}{figure.caption.51}}
\newlabel{Fig:9}{{2.13}{19}{Self Attention Weights \relax }{figure.caption.51}{}}
\@writefile{toc}{\contentsline {subsubsection}{II. Decoder}{19}{section*.52}}
\newlabel{bg:s4_sub3_subsub3}{{2.4.3}{19}{II. Decoder}{section*.52}{}}
\acronymused{RNN}
\acronymused{CNN}
\citation{devlin2018bert}
\citation{devlin2018bert}
\citation{Peters:2018}
\citation{howard2018universal}
\citation{devlin2018bert}
\citation{vaswani2017attention}
\citation{devlin2018bert}
\citation{gehring2017convolutional}
\citation{wu2016google}
\acronymused{BERT}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.4}Bidirectional Encoder from Transformer (BERT)}{20}{subsection.2.4.4}}
\newlabel{bg:s4_sub4}{{2.4.4}{20}{Bidirectional Encoder from Transformer (BERT)}{subsection.2.4.4}{}}
\acronymused{NLP}
\acronymused{BERT}
\acronymused{BERT}
\acronymused{BERT}
\@writefile{toc}{\contentsline {subsubsection}{Model Architecture and Input Representation}{20}{section*.53}}
\newlabel{bg:s4_sub4_subsub1}{{2.4.4}{20}{Model Architecture and Input Representation}{section*.53}{}}
\acronymused{BERT}
\acronymused{BERT}
\acronymused{NLP}
\@writefile{toc}{\contentsline {subsubsection}{Pre-training Tasks}{20}{section*.55}}
\newlabel{bg:s4_sub4_subsub2}{{2.4.4}{20}{Pre-training Tasks}{section*.55}{}}
\acronymused{BERT}
\citation{devlin2018bert}
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces BERT input representation\relax }}{21}{figure.caption.54}}
\newlabel{Fig:9}{{2.14}{21}{BERT input representation\relax }{figure.caption.54}{}}
\AC@undonewlabel{acro:MLM}
\newlabel{acro:MLM}{{1}{21}{Pre-training Tasks}{section*.56}{}}
\acronymused{MLM}
\acronymused{MLM}
\acronymused{BERT}
\acronymused{NLP}
\@setckpt{background}{
\setcounter{page}{22}
\setcounter{equation}{24}
\setcounter{enumi}{2}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{4}
\setcounter{subsection}{4}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{14}
\setcounter{table}{0}
\setcounter{parentequation}{0}
\setcounter{float@type}{8}
\setcounter{ContinuedFloat}{0}
\setcounter{KVtest}{0}
\setcounter{subfigure}{0}
\setcounter{subfigure@save}{0}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{subtable@save}{0}
\setcounter{lotdepth}{1}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{Item}{12}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{16}
\setcounter{ALC@unique}{0}
\setcounter{ALC@line}{0}
\setcounter{ALC@rem}{0}
\setcounter{ALC@depth}{0}
\setcounter{algorithm}{0}
\setcounter{section@level}{3}
}
