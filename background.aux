\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{hopfield1982neural}
\citation{jordan1986serial}
\citation{elman1990finding}
\citation{rumelhart1985learning}
\citation{werbos1988generalization}
\citation{bishop1995neural}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Literature Review}{3}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:background}{{2}{3}{Literature Review}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Natural Language Processing}{3}{section.2.1}}
\newlabel{bg:s1}{{2.1}{3}{Natural Language Processing}{section.2.1}{}}
\AC@undonewlabel{acro:NLP}
\newlabel{acro:NLP}{{2.1}{3}{Natural Language Processing}{section*.5}{}}
\acronymused{NLP}
\AC@undonewlabel{acro:NLG}
\newlabel{acro:NLG}{{2.1}{3}{Natural Language Processing}{section*.6}{}}
\acronymused{NLG}
\AC@undonewlabel{acro:NLU}
\newlabel{acro:NLU}{{2.1}{3}{Natural Language Processing}{section*.7}{}}
\acronymused{NLU}
\acronymused{NLG}
\acronymused{NLU}
\acronymused{NLP}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Artificial Neural Networks}{3}{section.2.2}}
\newlabel{bg:s2}{{2.2}{3}{Artificial Neural Networks}{section.2.2}{}}
\AC@undonewlabel{acro:ANNs}
\newlabel{acro:ANNs}{{2.2}{3}{Artificial Neural Networks}{section*.8}{}}
\acronymused{ANNs}
\acronymused{ANNs}
\acronymused{ANNs}
\acronymused{ANNs}
\citation{rumelhart1985learning}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces A neuron represented as a circle and the weighted edges as arrows. The activation function is a function of the sum of the weighted edges\relax }}{4}{figure.caption.9}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{Fig:1}{{2.1}{4}{A neuron represented as a circle and the weighted edges as arrows. The activation function is a function of the sum of the weighted edges\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Feed Forward Neural Networks}{4}{subsection.2.2.1}}
\newlabel{bg:sub1}{{2.2.1}{4}{Feed Forward Neural Networks}{subsection.2.2.1}{}}
\AC@undonewlabel{acro:FNNs}
\newlabel{acro:FNNs}{{2.2.1}{4}{Feed Forward Neural Networks}{section*.10}{}}
\acronymused{FNNs}
\AC@undonewlabel{acro:MLPs}
\newlabel{acro:MLPs}{{2.2.1}{4}{Feed Forward Neural Networks}{section*.11}{}}
\acronymused{MLPs}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces A simple Feed Forward Neural Network consisting of an input layer, one hidden layer, and an output layer\relax }}{4}{figure.caption.12}}
\newlabel{Fig:2}{{2.2}{4}{A simple Feed Forward Neural Network consisting of an input layer, one hidden layer, and an output layer\relax }{figure.caption.12}{}}
\acronymused{FNNs}
\citation{werbos1990backpropagation}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces A deep Feed Forward Neural Network consisting of many hidden layers\relax }}{5}{figure.caption.13}}
\newlabel{Fig:3}{{2.3}{5}{A deep Feed Forward Neural Network consisting of many hidden layers\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsubsection}{Sequence Models and the Problem with FNNs}{5}{section*.14}}
\newlabel{bg:subsub1}{{2.2.1}{5}{Sequence Models and the Problem with FNNs}{section*.14}{}}
\acronymused{FNNs}
\acronymused{FNNs}
\acronymused{FNNs}
\acronymused{FNNs}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Recurrent Neural Networks}{5}{subsection.2.2.2}}
\newlabel{bg:sub2}{{2.2.2}{5}{Recurrent Neural Networks}{subsection.2.2.2}{}}
\acronymused{ANNs}
\AC@undonewlabel{acro:RNNs}
\newlabel{acro:RNNs}{{2.2.2}{5}{Recurrent Neural Networks}{section*.16}{}}
\acronymused{RNNs}
\citation{schuster1997bidirectional}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces A Recurrent Neural Network\relax }}{6}{figure.caption.15}}
\newlabel{Fig:4}{{2.4}{6}{A Recurrent Neural Network\relax }{figure.caption.15}{}}
\acronymused{RNNs}
\acronymused{ANNs}
\AC@undonewlabel{acro:BPTT}
\newlabel{acro:BPTT}{{2.2.2}{6}{Recurrent Neural Networks}{section*.17}{}}
\acronymused{BPTT}
\newlabel{eq:1}{{2.1}{6}{Recurrent Neural Networks}{equation.2.2.1}{}}
\newlabel{eq:2}{{2.2}{6}{Recurrent Neural Networks}{equation.2.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{Bidirectional RNNs}{6}{section*.18}}
\newlabel{bg:subsub3}{{2.2.2}{6}{Bidirectional RNNs}{section*.18}{}}
\acronymused{RNNs}
\AC@undonewlabel{acro:BRNNs}
\newlabel{acro:BRNNs}{{2.2.2}{6}{Bidirectional RNNs}{section*.19}{}}
\acronymused{BRNNs}
\acronymused{BRNNs}
\acronymused{NLP}
\acronymused{BRNNs}
\acronymused{BRNNs}
\acronymused{BRNNs}
\acronymused{BRNNs}
\citation{hochreiter1991untersuchungen}
\citation{hochreiter2001gradient}
\citation{bengio1994learning}
\citation{hochreiter1997long}
\newlabel{eq:4}{{2.3}{7}{Bidirectional RNNs}{equation.2.2.3}{}}
\newlabel{eq:5}{{2.4}{7}{Bidirectional RNNs}{equation.2.2.4}{}}
\newlabel{eq:6}{{2.5}{7}{Bidirectional RNNs}{equation.2.2.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces A Bidirectional Recurrent Neural Network\relax }}{7}{figure.caption.20}}
\newlabel{Fig:5}{{2.5}{7}{A Bidirectional Recurrent Neural Network\relax }{figure.caption.20}{}}
\@writefile{toc}{\contentsline {subsubsection}{Problems with RNNs}{7}{section*.21}}
\newlabel{bg:subsub4}{{2.2.2}{7}{Problems with RNNs}{section*.21}{}}
\@writefile{toc}{\contentsline {subsubsection}{Long Short-Term Memory}{7}{section*.22}}
\newlabel{bg:subsub5}{{2.2.2}{7}{Long Short-Term Memory}{section*.22}{}}
\AC@undonewlabel{acro:LSTM}
\newlabel{acro:LSTM}{{2.2.2}{7}{Long Short-Term Memory}{section*.23}{}}
\acronymused{LSTM}
\acronymused{RNNs}
\newlabel{eq:10}{{2.6}{7}{Long Short-Term Memory}{equation.2.2.6}{}}
\newlabel{eq:7}{{2.7}{7}{Long Short-Term Memory}{equation.2.2.7}{}}
\newlabel{eq:8}{{2.8}{7}{Long Short-Term Memory}{equation.2.2.8}{}}
\newlabel{eq:9}{{2.9}{7}{Long Short-Term Memory}{equation.2.2.9}{}}
\newlabel{eq:11}{{2.10}{7}{Long Short-Term Memory}{equation.2.2.10}{}}
\newlabel{eq:12}{{2.11}{7}{Long Short-Term Memory}{equation.2.2.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces An LSTM Cell\relax }}{8}{figure.caption.24}}
\newlabel{Fig:6}{{2.6}{8}{An LSTM Cell\relax }{figure.caption.24}{}}
\@writefile{toc}{\contentsline {subsubsection}{Gated Recurrent Units}{8}{section*.25}}
\newlabel{bg:subsub6}{{2.2.2}{8}{Gated Recurrent Units}{section*.25}{}}
\AC@undonewlabel{acro:GRUs}
\newlabel{acro:GRUs}{{2.2.2}{8}{Gated Recurrent Units}{section*.26}{}}
\acronymused{GRUs}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Convolution Neural Networks}{8}{subsection.2.2.3}}
\newlabel{bg:sub3}{{2.2.3}{8}{Convolution Neural Networks}{subsection.2.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Speech Recognition}{8}{section.2.3}}
\newlabel{bg:s3}{{2.3}{8}{Speech Recognition}{section.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}End-to-End Speech Recognition}{8}{subsection.2.3.1}}
\newlabel{bg:sub4}{{2.3.1}{8}{End-to-End Speech Recognition}{subsection.2.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Feature Extraction}{8}{subsection.2.3.2}}
\newlabel{bg:sub5}{{2.3.2}{8}{Feature Extraction}{subsection.2.3.2}{}}
\acronymused{ASR}
\AC@undonewlabel{acro:MFCC}
\newlabel{acro:MFCC}{{2.3.2}{8}{Feature Extraction}{section*.27}{}}
\acronymused{MFCC}
\AC@undonewlabel{acro:FFT}
\newlabel{acro:FFT}{{2.3.2}{8}{Feature Extraction}{section*.28}{}}
\acronymused{FFT}
\AC@undonewlabel{acro:DCT}
\newlabel{acro:DCT}{{2.3.2}{8}{Feature Extraction}{section*.29}{}}
\acronymused{DCT}
\acronymused{DCT}
\acronymused{DCT}
\citation{graves2006connectionist}
\acronymused{ASR}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Connectionist Temporal Classification \cite  {graves2006connectionist}}{9}{subsection.2.3.3}}
\newlabel{bg:sub7}{{2.3.3}{9}{Connectionist Temporal Classification \cite {graves2006connectionist}}{subsection.2.3.3}{}}
\AC@undonewlabel{acro:RNN}
\newlabel{acro:RNN}{{2.3.3}{9}{Connectionist Temporal Classification \cite {graves2006connectionist}}{section*.30}{}}
\acronymused{RNN}
\acronymused{RNN}
\acronymused{RNN}
\AC@undonewlabel{acro:CTC}
\newlabel{acro:CTC}{{2.3.3}{9}{Connectionist Temporal Classification \cite {graves2006connectionist}}{section*.31}{}}
\acronymused{CTC}
\acronymused{RNN}
\acronymused{BPTT}
\acronymused{RNN}
\newlabel{eq:21}{{2.12}{9}{Connectionist Temporal Classification \cite {graves2006connectionist}}{equation.2.3.12}{}}
\newlabel{eq:22}{{2.13}{9}{Connectionist Temporal Classification \cite {graves2006connectionist}}{equation.2.3.13}{}}
\citation{cho2014learning}
\citation{sutskever2014sequence}
\citation{sutskever2014sequence}
\citation{cho2014learning}
\acronymused{CTC}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Text Analytics}{10}{section.2.4}}
\newlabel{bg:s4}{{2.4}{10}{Text Analytics}{section.2.4}{}}
\acronymused{RNN}
\acronymused{RNN}
\acronymused{RNN}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Encoder-Decoder Architecture}{10}{subsection.2.4.1}}
\newlabel{bg:sub8}{{2.4.1}{10}{Encoder-Decoder Architecture}{subsection.2.4.1}{}}
\acronymused{RNNs}
\acronymused{RNN}
\acronymused{LSTM}
\acronymused{LSTM}
\newlabel{eq:13}{{2.14}{10}{Encoder-Decoder Architecture}{equation.2.4.14}{}}
\citation{cho2014properties}
\citation{bahdanau2014neural}
\citation{bahdanau2014neural}
\citation{bahdanau2014neural}
\acronymused{RNN}
\newlabel{eq:14}{{2.15}{11}{Encoder-Decoder Architecture}{equation.2.4.15}{}}
\newlabel{eq:15}{{2.16}{11}{Encoder-Decoder Architecture}{equation.2.4.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Attention Mechanism}{11}{subsection.2.4.2}}
\newlabel{bg:sub9}{{2.4.2}{11}{Attention Mechanism}{subsection.2.4.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\MakeUppercase  {i}. Encoder}{11}{section*.32}}
\newlabel{bg:subsub9}{{2.4.2}{11}{\RomanNumeralCaps {1}. Encoder}{section*.32}{}}
\AC@undonewlabel{acro:BRNN}
\newlabel{acro:BRNN}{{2.4.2}{11}{\RomanNumeralCaps {1}. Encoder}{section*.33}{}}
\acronymused{BRNN}
\@writefile{toc}{\contentsline {subsubsection}{\MakeUppercase  {ii}. Decoder}{11}{section*.34}}
\newlabel{bg:subsub10}{{2.4.2}{11}{\RomanNumeralCaps {2}. Decoder}{section*.34}{}}
\acronymused{RNN}
\citation{he2016deep}
\citation{ba2016layer}
\newlabel{eq:16}{{2.17}{12}{\RomanNumeralCaps {2}. Decoder}{equation.2.4.17}{}}
\newlabel{eq:17}{{2.18}{12}{\RomanNumeralCaps {2}. Decoder}{equation.2.4.18}{}}
\newlabel{eq:18}{{2.19}{12}{\RomanNumeralCaps {2}. Decoder}{equation.2.4.19}{}}
\newlabel{eq:19}{{2.20}{12}{\RomanNumeralCaps {2}. Decoder}{equation.2.4.20}{}}
\newlabel{eq:20}{{2.21}{12}{\RomanNumeralCaps {2}. Decoder}{equation.2.4.21}{}}
\@writefile{toc}{\contentsline {subsubsection}{\MakeUppercase  {iii}. Additive Attention}{12}{section*.35}}
\newlabel{bg:subsub100}{{2.4.2}{12}{\RomanNumeralCaps {3}. Additive Attention}{section*.35}{}}
\acronymused{RNN}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}The Transformer}{12}{subsection.2.4.3}}
\newlabel{bg:sub10}{{2.4.3}{12}{The Transformer}{subsection.2.4.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\MakeUppercase  {i}. Problem with Recurrence}{12}{section*.36}}
\newlabel{bg:subsub11}{{2.4.3}{12}{\RomanNumeralCaps {1}. Problem with Recurrence}{section*.36}{}}
\acronymused{RNNs}
\@writefile{toc}{\contentsline {subsubsection}{\MakeUppercase  {ii}. Encoder}{12}{section*.37}}
\newlabel{bg:subsub12}{{2.4.3}{12}{\RomanNumeralCaps {2}. Encoder}{section*.37}{}}
\@writefile{toc}{\contentsline {subsubsection}{\MakeUppercase  {iii}. Decoder}{13}{section*.38}}
\newlabel{bg:subsub13}{{2.4.3}{13}{\RomanNumeralCaps {3}. Decoder}{section*.38}{}}
\@writefile{toc}{\contentsline {subsubsection}{\MakeUppercase  {iv}. Scaled Dot-Product Attention}{13}{section*.39}}
\newlabel{bg:subsub14}{{2.4.3}{13}{\RomanNumeralCaps {4}. Scaled Dot-Product Attention}{section*.39}{}}
\@writefile{toc}{\contentsline {subsubsection}{\MakeUppercase  {v}. Positional Encodings}{13}{section*.40}}
\newlabel{bg:subsub15}{{2.4.3}{13}{\RomanNumeralCaps {5}. Positional Encodings}{section*.40}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.4}Bidirectional Encoder from Transformer (BERT)}{13}{subsection.2.4.4}}
\newlabel{bg:sub11}{{2.4.4}{13}{Bidirectional Encoder from Transformer (BERT)}{subsection.2.4.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Word Embeddings}{13}{section*.41}}
\@writefile{toc}{\contentsline {subsubsection}{Transfer Learning}{13}{section*.42}}
\@writefile{toc}{\contentsline {paragraph}{Feature Extraction}{13}{section*.43}}
\@writefile{toc}{\contentsline {paragraph}{Fine Tuning}{13}{section*.44}}
\@writefile{toc}{\contentsline {subsubsection}{Bidirectional vs uni-directional}{13}{section*.45}}
\@writefile{toc}{\contentsline {subsubsection}{PreTraining Tasks}{13}{section*.46}}
\@setckpt{background}{
\setcounter{page}{14}
\setcounter{equation}{21}
\setcounter{enumi}{2}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{4}
\setcounter{subsection}{4}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{6}
\setcounter{table}{0}
\setcounter{parentequation}{0}
\setcounter{float@type}{8}
\setcounter{ContinuedFloat}{0}
\setcounter{KVtest}{0}
\setcounter{subfigure}{0}
\setcounter{subfigure@save}{0}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{subtable@save}{0}
\setcounter{lotdepth}{1}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{Item}{4}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{19}
\setcounter{ALC@unique}{0}
\setcounter{ALC@line}{0}
\setcounter{ALC@rem}{0}
\setcounter{ALC@depth}{0}
\setcounter{algorithm}{0}
\setcounter{section@level}{3}
}
