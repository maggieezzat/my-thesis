\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{hopfield1982neural}
\citation{jordan1986serial}
\citation{elman1990finding}
\citation{A critical review of recurrent neural networks for sequence learning}
\citation{A critical review of recurrent neural networks for sequence learning}
\citation{rumelhart1985learning}
\citation{werbos1988generalization}
\citation{bishop1995neural}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Literature Review}{3}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:background}{{2}{3}{Literature Review}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Natural Language Processing}{3}{section.2.1}}
\newlabel{bg:s1}{{2.1}{3}{Natural Language Processing}{section.2.1}{}}
\AC@undonewlabel{acro:NLP}
\newlabel{acro:NLP}{{2.1}{3}{Natural Language Processing}{section*.5}{}}
\acronymused{NLP}
\AC@undonewlabel{acro:NLG}
\newlabel{acro:NLG}{{2.1}{3}{Natural Language Processing}{section*.6}{}}
\acronymused{NLG}
\AC@undonewlabel{acro:NLU}
\newlabel{acro:NLU}{{2.1}{3}{Natural Language Processing}{section*.7}{}}
\acronymused{NLU}
\acronymused{NLG}
\acronymused{NLU}
\acronymused{NLP}
\acronymused{ASR}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Artificial Neural Networks}{3}{section.2.2}}
\newlabel{bg:s2}{{2.2}{3}{Artificial Neural Networks}{section.2.2}{}}
\AC@undonewlabel{acro:ANN}
\newlabel{acro:ANN}{{2.2}{3}{Artificial Neural Networks}{section*.8}{}}
\acronymused{ANN}
\acronymused{ANN}
\acronymused{ANN}
\acronymused{ANN}
\citation{rumelhart1985learning}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces A neuron represented as a circle and the weighted edges as arrows. The activation function is a function of the sum of the weighted edges. \cite  {A critical review of recurrent neural networks for sequence learning}\relax }}{4}{figure.caption.9}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{Fig:1}{{2.1}{4}{A neuron represented as a circle and the weighted edges as arrows. The activation function is a function of the sum of the weighted edges. \cite {A critical review of recurrent neural networks for sequence learning}\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Feed Forward Neural Networks}{4}{subsection.2.2.1}}
\newlabel{bg:sub1}{{2.2.1}{4}{Feed Forward Neural Networks}{subsection.2.2.1}{}}
\AC@undonewlabel{acro:FNN}
\newlabel{acro:FNN}{{2.2.1}{4}{Feed Forward Neural Networks}{section*.10}{}}
\acronymused{FNN}
\AC@undonewlabel{acro:MLP}
\newlabel{acro:MLP}{{2.2.1}{4}{Feed Forward Neural Networks}{section*.11}{}}
\acronymused{MLP}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces A simple Feed Forward Neural Network consisting of an input layer, one hidden layer, and an output layer\relax }}{4}{figure.caption.12}}
\newlabel{Fig:2}{{2.2}{4}{A simple Feed Forward Neural Network consisting of an input layer, one hidden layer, and an output layer\relax }{figure.caption.12}{}}
\acronymused{FNN}
\citation{werbos1990backpropagation}
\acronymused{FNN}
\@writefile{toc}{\contentsline {subsubsection}{Sequence Models and the Problem with \ac {FNN}s}{5}{section*.13}}
\newlabel{bg:subsub1}{{2.2.1}{5}{Sequence Models and the Problem with \ac {FNN}s}{section*.13}{}}
\acronymused{FNN}
\acronymused{FNN}
\acronymused{FNN}
\acronymused{FNN}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Recurrent Neural Networks}{5}{subsection.2.2.2}}
\newlabel{bg:sub2}{{2.2.2}{5}{Recurrent Neural Networks}{subsection.2.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces A Recurrent Neural Network\relax }}{5}{figure.caption.14}}
\newlabel{Fig:4}{{2.3}{5}{A Recurrent Neural Network\relax }{figure.caption.14}{}}
\acronymused{ANN}
\AC@undonewlabel{acro:RNN}
\newlabel{acro:RNN}{{2.2.2}{5}{Recurrent Neural Networks}{section*.15}{}}
\acronymused{RNN}
\acronymused{RNN}
\citation{schuster1997bidirectional}
\citation{hochreiter1991untersuchungen}
\citation{hochreiter2001gradient}
\citation{bengio1994learning}
\citation{hochreiter1997long}
\acronymused{ANN}
\AC@undonewlabel{acro:BPTT}
\newlabel{acro:BPTT}{{2.2.2}{6}{Recurrent Neural Networks}{section*.16}{}}
\acronymused{BPTT}
\newlabel{eq:1}{{2.1}{6}{Recurrent Neural Networks}{equation.2.2.1}{}}
\newlabel{eq:2}{{2.2}{6}{Recurrent Neural Networks}{equation.2.2.2}{}}
\acronymused{RNN}
\@writefile{toc}{\contentsline {subsubsection}{Bidirectional \ac {RNN}s}{6}{section*.17}}
\newlabel{bg:subsub3}{{2.2.2}{6}{Bidirectional \ac {RNN}s}{section*.17}{}}
\acronymused{RNN}
\AC@undonewlabel{acro:BRNN}
\newlabel{acro:BRNN}{{2.2.2}{6}{Bidirectional \ac {RNN}s}{section*.18}{}}
\acronymused{BRNN}
\acronymused{BRNN}
\acronymused{BRNN}
\acronymused{BRNN}
\acronymused{BRNN}
\acronymused{BRNN}
\newlabel{eq:4}{{2.3}{6}{Bidirectional \ac {RNN}s}{equation.2.2.3}{}}
\newlabel{eq:5}{{2.4}{6}{Bidirectional \ac {RNN}s}{equation.2.2.4}{}}
\newlabel{eq:6}{{2.5}{6}{Bidirectional \ac {RNN}s}{equation.2.2.5}{}}
\acronymused{RNN}
\@writefile{toc}{\contentsline {subsubsection}{Problems with \ac {RNN}s}{6}{section*.20}}
\newlabel{bg:subsub4}{{2.2.2}{6}{Problems with \ac {RNN}s}{section*.20}{}}
\@writefile{toc}{\contentsline {subsubsection}{Long Short-Term Memory}{6}{section*.21}}
\newlabel{bg:subsub5}{{2.2.2}{6}{Long Short-Term Memory}{section*.21}{}}
\AC@undonewlabel{acro:LSTM}
\newlabel{acro:LSTM}{{2.2.2}{6}{Long Short-Term Memory}{section*.22}{}}
\acronymused{LSTM}
\acronymused{RNN}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces A Bidirectional Recurrent Neural Network\relax }}{7}{figure.caption.19}}
\newlabel{Fig:5}{{2.4}{7}{A Bidirectional Recurrent Neural Network\relax }{figure.caption.19}{}}
\newlabel{eq:10}{{2.6}{7}{Long Short-Term Memory}{equation.2.2.6}{}}
\newlabel{eq:7}{{2.7}{7}{Long Short-Term Memory}{equation.2.2.7}{}}
\newlabel{eq:8}{{2.8}{7}{Long Short-Term Memory}{equation.2.2.8}{}}
\newlabel{eq:9}{{2.9}{7}{Long Short-Term Memory}{equation.2.2.9}{}}
\newlabel{eq:11}{{2.10}{7}{Long Short-Term Memory}{equation.2.2.10}{}}
\newlabel{eq:12}{{2.11}{7}{Long Short-Term Memory}{equation.2.2.11}{}}
\acronymused{LSTM}
\@writefile{toc}{\contentsline {subsubsection}{Problems with \ac {LSTM}}{7}{section*.24}}
\newlabel{conc:subsub7}{{2.2.2}{7}{Problems with \ac {LSTM}}{section*.24}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Convolution Neural Networks}{7}{subsection.2.2.3}}
\newlabel{bg:sub3}{{2.2.3}{7}{Convolution Neural Networks}{subsection.2.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces An \ac {LSTM} Cell\relax }}{8}{figure.caption.23}}
\acronymused{LSTM}
\newlabel{Fig:6}{{2.5}{8}{An \ac {LSTM} Cell\relax }{figure.caption.23}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Speech Recognition}{8}{section.2.3}}
\newlabel{bg:s3}{{2.3}{8}{Speech Recognition}{section.2.3}{}}
\AC@undonewlabel{acro:HMM}
\newlabel{acro:HMM}{{2.3}{8}{Speech Recognition}{section*.25}{}}
\acronymused{HMM}
\acronymused{ASR}
\acronymused{HMM}
\acronymused{ASR}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}HMM-Based \ac {ASR} Systems}{8}{subsection.2.3.1}}
\newlabel{bg:sub4}{{2.3.1}{8}{HMM-Based \ac {ASR} Systems}{subsection.2.3.1}{}}
\acronymused{ASR}
\acronymused{ASR}
\newlabel{eq:24}{{2.12}{8}{HMM-Based \ac {ASR} Systems}{equation.2.3.12}{}}
\acronymused{ASR}
\acronymused{ASR}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces An ASR System\relax }}{9}{figure.caption.26}}
\newlabel{Fig:7}{{2.6}{9}{An ASR System\relax }{figure.caption.26}{}}
\acronymused{HMM}
\@writefile{toc}{\contentsline {subsubsection}{Feature Extraction}{9}{section*.27}}
\newlabel{bg:sub5}{{2.3.1}{9}{Feature Extraction}{section*.27}{}}
\acronymused{ASR}
\AC@undonewlabel{acro:MFCC}
\newlabel{acro:MFCC}{{2.3.1}{9}{Feature Extraction}{section*.29}{}}
\acronymused{MFCC}
\AC@undonewlabel{acro:FFT}
\newlabel{acro:FFT}{{2.3.1}{9}{Feature Extraction}{section*.30}{}}
\acronymused{FFT}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces MFCC\relax }}{10}{figure.caption.28}}
\newlabel{Fig:7}{{2.7}{10}{MFCC\relax }{figure.caption.28}{}}
\AC@undonewlabel{acro:DCT}
\newlabel{acro:DCT}{{2.3.1}{10}{Feature Extraction}{section*.31}{}}
\acronymused{DCT}
\acronymused{DCT}
\acronymused{DCT}
\acronymused{ASR}
\@writefile{toc}{\contentsline {subsubsection}{Acoustic Model}{10}{section*.32}}
\newlabel{bg:subsub6}{{2.3.1}{10}{Acoustic Model}{section*.32}{}}
\acronymused{HMM}
\acronymused{HMM}
\acronymused{HMM}
\AC@undonewlabel{acro:FSM}
\newlabel{acro:FSM}{{2.3.1}{10}{Acoustic Model}{section*.33}{}}
\acronymused{FSM}
\acronymused{HMM}
\acronymused{HMM}
\citation{keselj2009speech}
\citation{rabiner1989tutorial}
\citation{keselj2009speech}
\citation{baum1972inequality}
\acronymused{HMM}
\acronymused{HMM}
\acronymused{HMM}
\citation{graves2006connectionist}
\@writefile{toc}{\contentsline {subsubsection}{Language Model}{12}{section*.34}}
\newlabel{bg:subsub5}{{2.3.1}{12}{Language Model}{section*.34}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Hybrid Systems}{12}{subsection.2.3.2}}
\newlabel{bg:sub6}{{2.3.2}{12}{Hybrid Systems}{subsection.2.3.2}{}}
\acronymused{RNN}
\acronymused{RNN}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}End-to-End Systems}{12}{subsection.2.3.3}}
\newlabel{bg:sub7}{{2.3.3}{12}{End-to-End Systems}{subsection.2.3.3}{}}
\acronymused{RNN}
\AC@undonewlabel{acro:CTC}
\newlabel{acro:CTC}{{2.3.3}{12}{End-to-End Systems}{section*.35}{}}
\acronymused{CTC}
\acronymused{RNN}
\acronymused{BPTT}
\acronymused{RNN}
\newlabel{eq:21}{{2.14}{12}{End-to-End Systems}{equation.2.3.14}{}}
\newlabel{eq:22}{{2.15}{13}{End-to-End Systems}{equation.2.3.15}{}}
\acronymused{CTC}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Deep Speech 2}{13}{subsection.2.3.4}}
\newlabel{bg:sub8}{{2.3.4}{13}{Deep Speech 2}{subsection.2.3.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Text Analysis}{13}{section.2.4}}
\newlabel{bg:s4}{{2.4}{13}{Text Analysis}{section.2.4}{}}
\acronymused{NLP}
\citation{cho2014learning}
\citation{sutskever2014sequence}
\citation{sutskever2014sequence}
\citation{cho2014learning}
\AC@undonewlabel{acro:BERT}
\newlabel{acro:BERT}{{2.4}{14}{Text Analysis}{section*.36}{}}
\acronymused{BERT}
\acronymused{BERT}
\acronymused{BERT}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Encoder-Decoder Architecture}{14}{subsection.2.4.1}}
\newlabel{bg:sub9}{{2.4.1}{14}{Encoder-Decoder Architecture}{subsection.2.4.1}{}}
\acronymused{RNN}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces An Encoder-Decoder System. The encoder encodes a variable-length sequence into a fixed-length vector $\mathbf  {c}$. The decoder uses the summary vector $\mathbf  {c}$ along with the previously generated predicted symbol from the previous time step $y_{t-1}$ and the current hidden state $\mathbf  {s_t}$ to generate an output $y_t$ \relax }}{14}{figure.caption.37}}
\newlabel{Fig:7}{{2.8}{14}{An Encoder-Decoder System. The encoder encodes a variable-length sequence into a fixed-length vector $\mathbf {c}$. The decoder uses the summary vector $\mathbf {c}$ along with the previously generated predicted symbol from the previous time step $y_{t-1}$ and the current hidden state $\mathbf {s_t}$ to generate an output $y_t$ \relax }{figure.caption.37}{}}
\acronymused{RNN}
\acronymused{LSTM}
\acronymused{LSTM}
\newlabel{eq:13}{{2.16}{14}{Encoder-Decoder Architecture}{equation.2.4.16}{}}
\acronymused{RNN}
\citation{cho2014properties}
\citation{bahdanau2014neural}
\citation{bahdanau2014neural}
\citation{bahdanau2014neural}
\newlabel{eq:14}{{2.17}{15}{Encoder-Decoder Architecture}{equation.2.4.17}{}}
\newlabel{eq:15}{{2.18}{15}{Encoder-Decoder Architecture}{equation.2.4.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Attention Mechanism}{15}{subsection.2.4.2}}
\newlabel{bg:sub10}{{2.4.2}{15}{Attention Mechanism}{subsection.2.4.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\MakeUppercase  {i}. Encoder}{15}{section*.39}}
\newlabel{bg:subsub9}{{2.4.2}{15}{\RomanNumeralCaps {1}. Encoder}{section*.39}{}}
\acronymused{BRNN}
\@writefile{toc}{\contentsline {subsubsection}{\MakeUppercase  {ii}. Decoder}{15}{section*.40}}
\newlabel{bg:subsub10}{{2.4.2}{15}{\RomanNumeralCaps {2}. Decoder}{section*.40}{}}
\acronymused{RNN}
\citation{vaswani2017attention}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces The modified encoder-decoder system implementing the attention mechanism. The output $y_i$ has a corresponding context vector $\mathbf  {c_i}$ which is a summation of the weighted annotations $h_j$. The most relevant annotations in the input sentence have the largest weights $\alpha _{ij}$, while the least relevant annotations have the smallest weights. \relax }}{16}{figure.caption.38}}
\newlabel{Fig:8}{{2.9}{16}{The modified encoder-decoder system implementing the attention mechanism. The output $y_i$ has a corresponding context vector $\mathbf {c_i}$ which is a summation of the weighted annotations $h_j$. The most relevant annotations in the input sentence have the largest weights $\alpha _{ij}$, while the least relevant annotations have the smallest weights. \relax }{figure.caption.38}{}}
\newlabel{eq:16}{{2.19}{16}{\RomanNumeralCaps {2}. Decoder}{equation.2.4.19}{}}
\newlabel{eq:17}{{2.20}{16}{\RomanNumeralCaps {2}. Decoder}{equation.2.4.20}{}}
\newlabel{eq:18}{{2.21}{16}{\RomanNumeralCaps {2}. Decoder}{equation.2.4.21}{}}
\newlabel{eq:19}{{2.22}{16}{\RomanNumeralCaps {2}. Decoder}{equation.2.4.22}{}}
\acronymused{FNN}
\acronymused{RNN}
\citation{he2016deep}
\citation{ba2016layer}
\citation{gehring2017convolutional}
\citation{vaswani2017attention}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}The Transformer}{17}{subsection.2.4.3}}
\newlabel{bg:sub12}{{2.4.3}{17}{The Transformer}{subsection.2.4.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{Problem with Recurrence}{17}{section*.41}}
\newlabel{bg:subsub11}{{2.4.3}{17}{Problem with Recurrence}{section*.41}{}}
\acronymused{RNN}
\@writefile{toc}{\contentsline {subsubsection}{ Model Architecture}{17}{section*.42}}
\newlabel{bg:subsub12}{{2.4.3}{17}{Model Architecture}{section*.42}{}}
\citation{bahdanau2014neural}
\acronymused{RNN}
\AC@undonewlabel{acro:CNN}
\newlabel{acro:CNN}{{2.4.3}{18}{Model Architecture}{section*.43}{}}
\acronymused{CNN}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.4}Bidirectional Encoder from Transformer (BERT)}{18}{subsection.2.4.4}}
\newlabel{bg:sub13}{{2.4.4}{18}{Bidirectional Encoder from Transformer (BERT)}{subsection.2.4.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Bidirectional vs uni-directional}{18}{section*.44}}
\@writefile{toc}{\contentsline {subsubsection}{PreTraining Tasks}{18}{section*.45}}
\@setckpt{background}{
\setcounter{page}{19}
\setcounter{equation}{22}
\setcounter{enumi}{3}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{4}
\setcounter{subsection}{4}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{9}
\setcounter{table}{0}
\setcounter{parentequation}{0}
\setcounter{float@type}{8}
\setcounter{ContinuedFloat}{0}
\setcounter{KVtest}{0}
\setcounter{subfigure}{0}
\setcounter{subfigure@save}{0}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{subtable@save}{0}
\setcounter{lotdepth}{1}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{Item}{14}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{18}
\setcounter{ALC@unique}{0}
\setcounter{ALC@line}{0}
\setcounter{ALC@rem}{0}
\setcounter{ALC@depth}{0}
\setcounter{algorithm}{0}
\setcounter{section@level}{3}
}
