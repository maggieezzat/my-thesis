\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Methodology}{23}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:methodology}{{3}{23}{Methodology}{chapter.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Automated Dispatcher Actions System comprising two sub-systems: Automatic Speech Recognition unit taking input as raw speech from the driver. The text output is fed into a trained Text Classifier which issues the proper actions accordingly.\relax }}{23}{figure.caption.57}}
\newlabel{meth:fig1}{{3.1}{23}{Automated Dispatcher Actions System comprising two sub-systems: Automatic Speech Recognition unit taking input as raw speech from the driver. The text output is fed into a trained Text Classifier which issues the proper actions accordingly.\relax }{figure.caption.57}{}}
\citation{daniel2011kaldi}
\citation{milde2018open}
\citation{daniel2011kaldi}
\citation{milde2018open}
\citation{waibel1990readings}
\citation{peddinti2015time}
\citation{rousseau2014enhancing}
\citation{milde2018open}
\citation{radeck2015open}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}System Overview}{24}{section.3.1}}
\newlabel{meth:s1}{{3.1}{24}{System Overview}{section.3.1}{}}
\acronymused{ASR}
\acronymused{ASR}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Two sub-systems vs. One speech-based End-to-End System}{24}{subsection.3.1.1}}
\newlabel{meth:s1_sub1}{{3.1.1}{24}{Two sub-systems vs. One speech-based End-to-End System}{subsection.3.1.1}{}}
\acronymused{MFCC}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Automatic Speech Recognition Unit}{24}{section.3.2}}
\newlabel{meth:s2}{{3.2}{24}{Automatic Speech Recognition Unit}{section.3.2}{}}
\acronymused{ASR}
\acronymused{ASR}
\acronymused{ASR}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Hybrid Kaldi-based ASR Model}{24}{subsection.3.2.1}}
\newlabel{meth:s2_sub1}{{3.2.1}{24}{Hybrid Kaldi-based ASR Model}{subsection.3.2.1}{}}
\acronymused{ASR}
\acronymused{ASR}
\acronymused{ASR}
\AC@undonewlabel{acro:GMM}
\newlabel{acro:GMM}{{3.2.1}{24}{Hybrid Kaldi-based ASR Model}{section*.58}{}}
\acronymused{GMM}
\acronymused{HMM}
\AC@undonewlabel{acro:TDNN}
\newlabel{acro:TDNN}{{3.2.1}{24}{Hybrid Kaldi-based ASR Model}{section*.59}{}}
\acronymused{TDNN}
\AC@undonewlabel{acro:SWC}
\newlabel{acro:SWC}{{3.2.1}{24}{Hybrid Kaldi-based ASR Model}{section*.60}{}}
\acronymused{SWC}
\citation{milde2018open}
\citation{milde2018open}
\citation{milde2018open}
\citation{amodei2016deep}
\AC@undonewlabel{acro:WER}
\newlabel{acro:WER}{{3.2.1}{25}{Hybrid Kaldi-based ASR Model}{section*.61}{}}
\acronymused{WER}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Why End-to-End ASR?}{25}{subsection.3.2.2}}
\newlabel{meth:s2_sub2}{{3.2.2}{25}{Why End-to-End ASR?}{subsection.3.2.2}{}}
\acronymused{HMM}
\acronymused{ASR}
\acronymused{ASR}
\acronymused{HMM}
\acronymused{ASR}
\acronymused{ASR}
\acronymused{ASR}
\acronymused{ASR}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Deep Speech 2: Adapting to German}{25}{subsection.3.2.3}}
\newlabel{meth:s2_sub3}{{3.2.3}{25}{Deep Speech 2: Adapting to German}{subsection.3.2.3}{}}
\acronymused{ASR}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Speech Recognition Data}{25}{subsection.3.2.4}}
\newlabel{meth:s2_sub4}{{3.2.4}{25}{Speech Recognition Data}{subsection.3.2.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\MakeUppercase  {i}. Common Voice}{25}{section*.62}}
\newlabel{meth:s2_sub4_subsub1}{{3.2.4}{25}{\RomanNumeralCaps {1}. Common Voice}{section*.62}{}}
\acronymused{ASR}
\citation{radeck2015open}
\@writefile{toc}{\contentsline {subsubsection}{\MakeUppercase  {ii}. M-AILABS Speech Dataset}{26}{section*.63}}
\newlabel{meth:s2_sub4_subsub2}{{3.2.4}{26}{\RomanNumeralCaps {2}. M-AILABS Speech Dataset}{section*.63}{}}
\@writefile{toc}{\contentsline {subsubsection}{\MakeUppercase  {iii}. German Speech Data (Tuda-De)}{26}{section*.64}}
\newlabel{meth:s2_sub4_subsub3}{{3.2.4}{26}{\RomanNumeralCaps {3}. German Speech Data (Tuda-De)}{section*.64}{}}
\@writefile{toc}{\contentsline {subsubsection}{\MakeUppercase  {iv}. CSS10: Single Speaker Data}{26}{section*.65}}
\newlabel{meth:s2_sub4_subsub4}{{3.2.4}{26}{\RomanNumeralCaps {4}. CSS10: Single Speaker Data}{section*.65}{}}
\@writefile{toc}{\contentsline {subsubsection}{\MakeUppercase  {v}. Movies Data}{26}{section*.66}}
\newlabel{meth:s2_sub4_subsub5}{{3.2.4}{26}{\RomanNumeralCaps {5}. Movies Data}{section*.66}{}}
\acronymused{ASR}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Format of \texttt  {srt} Files\relax }}{27}{figure.caption.67}}
\newlabel{meth:fig2}{{3.2}{27}{Format of \texttt {srt} Files\relax }{figure.caption.67}{}}
\@writefile{toc}{\contentsline {subsubsection}{\MakeUppercase  {v}. Spoken Wikipedia Corpus}{27}{section*.68}}
\newlabel{meth:s2_sub4_subsub6}{{3.2.4}{27}{\RomanNumeralCaps {5}. Spoken Wikipedia Corpus}{section*.68}{}}
\acronymused{SWC}
\acronymused{SWC}
\acronymused{SWC}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces \acf {SWC} summary.\relax }}{27}{table.caption.69}}
\acronymused{SWC}
\newlabel{meth:table0}{{3.1}{27}{\acf {SWC} summary.\relax }{table.caption.69}{}}
\citation{tensorflow2015-whitepaper}
\@writefile{toc}{\contentsline {subsubsection}{Transcriptions Cleaning}{28}{section*.70}}
\newlabel{meth:s2_sub4_subsub7}{{3.2.4}{28}{Transcriptions Cleaning}{section*.70}{}}
\acronymused{ASR}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Deep Speech 2: TensorFlow}{28}{section.3.3}}
\newlabel{meth:s3}{{3.3}{28}{Deep Speech 2: TensorFlow}{section.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}TensorFlow GPU}{28}{subsection.3.3.1}}
\newlabel{meth:s3_sub1}{{3.3.1}{28}{TensorFlow GPU}{subsection.3.3.1}{}}
\AC@undonewlabel{acro:TPU}
\newlabel{acro:TPU}{{3.3.1}{28}{TensorFlow GPU}{section*.71}{}}
\acronymused{TPU}
\acronymused{TPU}
\AC@undonewlabel{acro:ASIC}
\newlabel{acro:ASIC}{{3.3.1}{28}{TensorFlow GPU}{section*.72}{}}
\acronymused{ASIC}
\@writefile{toc}{\contentsline {subsubsection}{Implementation Details: Estimators}{28}{section*.73}}
\newlabel{meth:s3_sub1_subsub1}{{3.3.1}{28}{Implementation Details: Estimators}{section*.73}{}}
\acronymused{TPU}
\@writefile{toc}{\contentsline {subsubsection}{Training Data, Process and Results}{29}{section*.74}}
\newlabel{meth:s3_sub1_subsub2}{{3.3.1}{29}{Training Data, Process and Results}{section*.74}{}}
\acronymused{GRU}
\acronymused{LSTM}
\acronymused{RNN}
\acronymused{WER}
\AC@undonewlabel{acro:CER}
\newlabel{acro:CER}{{3.3.1}{29}{Training Data, Process and Results}{section*.75}{}}
\acronymused{CER}
\acronymused{WER}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}TensorFlow TPU}{29}{subsection.3.3.2}}
\newlabel{meth:s3_sub2}{{3.3.2}{29}{TensorFlow TPU}{subsection.3.3.2}{}}
\acronymused{TPU}
\acronymused{TPU}
\acronymused{TPU}
\AC@undonewlabel{acro:HBM}
\newlabel{acro:HBM}{{3.3.2}{29}{TensorFlow TPU}{section*.76}{}}
\acronymused{HBM}
\acronymused{TPU}
\@writefile{toc}{\contentsline {subsubsection}{Migrating from Regular Estimator to TPU Estimator}{29}{section*.77}}
\newlabel{meth:s3_sub2_subsub1}{{3.3.2}{29}{Migrating from Regular Estimator to TPU Estimator}{section*.77}{}}
\acronymused{TPU}
\AC@undonewlabel{acro:XLA}
\newlabel{acro:XLA}{{3.3.2}{29}{Migrating from Regular Estimator to TPU Estimator}{section*.78}{}}
\acronymused{XLA}
\acronymused{TPU}
\acronymused{XLA}
\acronymused{TPU}
\acronymused{HBM}
\acronymused{TPU}
\acronymused{TPU}
\acronymused{TPU}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Deep Speech 2: PyTorch}{29}{section.3.4}}
\newlabel{meth:s4}{{3.4}{29}{Deep Speech 2: PyTorch}{section.3.4}{}}
\acronymused{TPU}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Training on Google Colab}{30}{subsection.3.4.1}}
\newlabel{meth:s4_sub1}{{3.4.1}{30}{Training on Google Colab}{subsection.3.4.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{LSTM vs. GRU}{30}{section*.79}}
\newlabel{meth:s4_sub1_subsub1}{{3.4.1}{30}{LSTM vs. GRU}{section*.79}{}}
\acronymused{GRU}
\acronymused{RNN}
\acronymused{GRU}
\acronymused{LSTM}
\acronymused{LSTM}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Microsoft Azure Platform}{30}{subsection.3.4.2}}
\newlabel{meth:s4_sub2}{{3.4.2}{30}{Microsoft Azure Platform}{subsection.3.4.2}{}}
\acronymused{TPU}
\AC@undonewlabel{acro:VM}
\newlabel{acro:VM}{{3.4.2}{30}{Microsoft Azure Platform}{section*.80}{}}
\acronymused{VM}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Training I: Using Tuda-De, M-AILABS, Common Voice }{30}{subsection.3.4.3}}
\newlabel{meth:s4_sub3}{{3.4.3}{30}{Training I: Using Tuda-De, M-AILABS, Common Voice}{subsection.3.4.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{Speaker Independent Splitting}{30}{section*.81}}
\newlabel{meth:s4_sub3_subsub1}{{3.4.3}{30}{Speaker Independent Splitting}{section*.81}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces Data used in the first 13 epochs. The number of hours per train, test and dev sets are shown.\relax }}{30}{table.caption.82}}
\newlabel{meth:table1}{{3.2}{30}{Data used in the first 13 epochs. The number of hours per train, test and dev sets are shown.\relax }{table.caption.82}{}}
\acronymused{ASR}
\citation{amodei2016deep}
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces Training summary for the sirst 13 epochs. The training data was Tuda-De, Common Voice and M-AILABS\relax }}{31}{table.caption.83}}
\newlabel{meth:table2}{{3.3}{31}{Training summary for the sirst 13 epochs. The training data was Tuda-De, Common Voice and M-AILABS\relax }{table.caption.83}{}}
\@writefile{toc}{\contentsline {subsubsection}{Training Process and Results}{31}{section*.85}}
\newlabel{meth:s4_sub3_subsub2}{{3.4.3}{31}{Training Process and Results}{section*.85}{}}
\acronymused{LSTM}
\acronymused{WER}
\acronymused{CER}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.4}Training II: Adding all data}{31}{subsection.3.4.4}}
\newlabel{meth:s4_sub4}{{3.4.4}{31}{Training II: Adding all data}{subsection.3.4.4}{}}
\acronymused{VM}
\acronymused{SWC}
\@writefile{toc}{\contentsline {subsubsection}{problem of adding \ac {SWC}}{31}{section*.86}}
\newlabel{meth:s4_sub4_subsub1}{{3.4.4}{31}{problem of adding \ac {SWC}}{section*.86}{}}
\acronymused{SWC}
\acronymused{SWC}
\@writefile{toc}{\contentsline {subsubsection}{Speaker Independent Splitting}{31}{section*.87}}
\newlabel{meth:s4_sub4_subsub2}{{3.4.4}{31}{Speaker Independent Splitting}{section*.87}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Training summary from epoch 1 to epoch 13. The graph shows the \ac {WER}, \ac {CER} and loss.\relax }}{32}{figure.caption.84}}
\acronymused{WER}
\acronymused{CER}
\newlabel{meth:fig3}{{3.3}{32}{Training summary from epoch 1 to epoch 13. The graph shows the \ac {WER}, \ac {CER} and loss.\relax }{figure.caption.84}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.4}{\ignorespaces Train, test and dev sets after adding CSS10 and movies data\relax }}{32}{table.caption.88}}
\newlabel{meth:table3}{{3.4}{32}{Train, test and dev sets after adding CSS10 and movies data\relax }{table.caption.88}{}}
\@writefile{toc}{\contentsline {subsubsection}{Training Process and Results}{32}{section*.91}}
\newlabel{meth:s4_sub4_subsub3}{{3.4.4}{32}{Training Process and Results}{section*.91}{}}
\acronymused{SWC}
\acronymused{WER}
\acronymused{CER}
\acronymused{CTC}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.5}Language Model Decoding}{32}{subsection.3.4.5}}
\newlabel{meth:s4_sub5}{{3.4.5}{32}{Language Model Decoding}{subsection.3.4.5}{}}
\acronymused{WER}
\citation{milde2018open}
\@writefile{lot}{\contentsline {table}{\numberline {3.5}{\ignorespaces Summary of the training Process. Movies data and CSS10 were added after epoch 13. Noise augmentation was also introduced after epoch 13\relax }}{33}{table.caption.89}}
\newlabel{meth:table4}{{3.5}{33}{Summary of the training Process. Movies data and CSS10 were added after epoch 13. Noise augmentation was also introduced after epoch 13\relax }{table.caption.89}{}}
\acronymused{NLP}
\acronymused{ASR}
\acronymused{ASR}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Training summary till epoch 19. The graph shows the \ac {WER}, \ac {CER} and the loss.\relax }}{34}{figure.caption.90}}
\acronymused{WER}
\acronymused{CER}
\newlabel{meth:fig4}{{3.4}{34}{Training summary till epoch 19. The graph shows the \ac {WER}, \ac {CER} and the loss.\relax }{figure.caption.90}{}}
\acronymused{ASR}
\newlabel{meth:eq1}{{3.1}{34}{Language Model Decoding}{equation.3.4.1}{}}
\acronymused{WER}
\acronymused{CER}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.6}Language Model Re-scoring}{34}{subsection.3.4.6}}
\newlabel{meth:s4_sub6}{{3.4.6}{34}{Language Model Re-scoring}{subsection.3.4.6}{}}
\citation{zhang2019automatic}
\citation{zhang2019automatic}
\citation{vaswani2017attention}
\citation{milde2018open}
\acronymused{WER}
\acronymused{WER}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.7}Auto Correct With Transformer}{35}{subsection.3.4.7}}
\newlabel{meth:s4_sub7}{{3.4.7}{35}{Auto Correct With Transformer}{subsection.3.4.7}{}}
\acronymused{CTC}
\acronymused{ASR}
\@writefile{toc}{\contentsline {subsubsection}{Model Selection and Defining the Problem}{35}{section*.92}}
\newlabel{meth:s4_sub7_subsub1}{{3.4.7}{35}{Model Selection and Defining the Problem}{section*.92}{}}
\AC@undonewlabel{acro:T2T}
\newlabel{acro:T2T}{{3.4.7}{35}{Model Selection and Defining the Problem}{section*.93}{}}
\acronymused{T2T}
\@writefile{toc}{\contentsline {subsubsection}{Training Data and Results}{35}{section*.94}}
\newlabel{meth:s4_sub7_subsub2}{{3.4.7}{35}{Training Data and Results}{section*.94}{}}
\acronymused{ASR}
\acronymused{ASR}
\acronymused{WER}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.8}Comparing with the Hybrid Model}{36}{subsection.3.4.8}}
\newlabel{meth:s4_sub8}{{3.4.8}{36}{Comparing with the Hybrid Model}{subsection.3.4.8}{}}
\acronymused{WER}
\acronymused{CER}
\acronymused{WER}
\acronymused{WER}
\acronymused{WER}
\acronymused{WER}
\acronymused{WER}
\citation{devlin2018bert}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Text Classifier Unit}{37}{section.3.5}}
\newlabel{meth:s5}{{3.5}{37}{Text Classifier Unit}{section.3.5}{}}
\acronymused{BERT}
\acronymused{BERT}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Why BERT?}{37}{subsection.3.5.1}}
\newlabel{meth:s5_sub1}{{3.5.1}{37}{Why BERT?}{subsection.3.5.1}{}}
\acronymused{BERT}
\acronymused{BERT}
\AC@undonewlabel{acro:MRPC}
\newlabel{acro:MRPC}{{3.5.1}{37}{Why BERT?}{section*.95}{}}
\acronymused{MRPC}
\acronymused{MRPC}
\acronymused{BERT}
\acronymused{BERT}
\acronymused{BERT}
\acronymused{BERT}
\acronymused{MLM}
\acronymused{BERT}
\acronymused{BERT}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Implementation}{37}{subsection.3.5.2}}
\newlabel{meth:s5_sub2}{{3.5.2}{37}{Implementation}{subsection.3.5.2}{}}
\acronymused{BERT}
\acronymused{BERT}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Pre-training Data}{37}{subsection.3.5.3}}
\newlabel{meth:s5_sub3}{{3.5.3}{37}{Pre-training Data}{subsection.3.5.3}{}}
\acronymused{BERT}
\@writefile{toc}{\contentsline {subsubsection}{Pre-processing German Wikipedia}{38}{section*.96}}
\newlabel{meth:s5_sub3_subsub1}{{3.5.3}{38}{Pre-processing German Wikipedia}{section*.96}{}}
\acronymused{ASR}
\acronymused{ASR}
\acronymused{ASR}
\acronymused{ASR}
\acronymused{NLP}
\acronymused{BERT}
\@writefile{toc}{\contentsline {subsubsection}{Generating Vocab File}{38}{section*.97}}
\newlabel{meth:s5_sub3_subsub2}{{3.5.3}{38}{Generating Vocab File}{section*.97}{}}
\acronymused{BERT}
\citation{kingma2014adam}
\citation{hendrycks2016bridging}
\acronymused{BERT}
\acronymused{BERT}
\@writefile{toc}{\contentsline {subsubsection}{Tokenization and Writing into TFRecord Format}{39}{section*.98}}
\newlabel{meth:s5_sub3_subsub3}{{3.5.3}{39}{Tokenization and Writing into TFRecord Format}{section*.98}{}}
\acronymused{MLM}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.4}Pre-Training Process}{39}{subsection.3.5.4}}
\newlabel{meth:s5_sub4}{{3.5.4}{39}{Pre-Training Process}{subsection.3.5.4}{}}
\acronymused{TPU}
\acronymused{TPU}
\acronymused{TPU}
\acronymused{TPU}
\acronymused{HBM}
\acronymused{TPU}
\acronymused{TPU}
\acronymused{MLM}
\acronymused{MLM}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.5}Fine-Tuning Data}{40}{subsection.3.5.5}}
\newlabel{meth:s5_sub5}{{3.5.5}{40}{Fine-Tuning Data}{subsection.3.5.5}{}}
\acronymused{BERT}
\AC@undonewlabel{acro:10KGNAD}
\newlabel{acro:10KGNAD}{{3.5.5}{40}{\ac {10KGNAD}}{section*.100}{}}
\acronymused{10KGNAD}
\@writefile{toc}{\contentsline {subsubsection}{\ac {10KGNAD}}{40}{section*.99}}
\newlabel{meth:s5_sub5_subsub1}{{3.5.5}{40}{\ac {10KGNAD}}{section*.99}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.6}{\ignorespaces 10KGNAD: Overview of train/test split\relax }}{40}{table.caption.101}}
\newlabel{meth:table5}{{3.6}{40}{10KGNAD: Overview of train/test split\relax }{table.caption.101}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces 10KGNAD: Articles per Class\relax }}{40}{figure.caption.102}}
\newlabel{meth:fig5}{{3.5}{40}{10KGNAD: Articles per Class\relax }{figure.caption.102}{}}
\acronymused{BERT}
\acronymused{10KGNAD}
\acronymused{ASR}
\acronymused{BERT}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.6}Fine-Tuning Process}{41}{subsection.3.5.6}}
\newlabel{meth:s5_sub6}{{3.5.6}{41}{Fine-Tuning Process}{subsection.3.5.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces The task specific models are formed by building an output layer on top of BERT, so that small number of parameters need to be learned from scratch. The final hidden layer corresponding to the special classification token is used as the classifier output. \relax }}{41}{figure.caption.103}}
\newlabel{meth:fig6}{{3.6}{41}{The task specific models are formed by building an output layer on top of BERT, so that small number of parameters need to be learned from scratch. The final hidden layer corresponding to the special classification token is used as the classifier output. \relax }{figure.caption.103}{}}
\acronymused{BERT}
\acronymused{BERT}
\acronymused{TPU}
\@writefile{toc}{\contentsline {subsubsection}{Effect of Running Pre-Training on Fine-Tuning Data}{42}{section*.104}}
\newlabel{meth:s5_sub6_subsub1}{{3.5.6}{42}{Effect of Running Pre-Training on Fine-Tuning Data}{section*.104}{}}
\acronymused{BERT}
\@setckpt{methodology}{
\setcounter{page}{43}
\setcounter{equation}{1}
\setcounter{enumi}{4}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{29}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{3}
\setcounter{section}{5}
\setcounter{subsection}{6}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{6}
\setcounter{table}{6}
\setcounter{parentequation}{0}
\setcounter{float@type}{8}
\setcounter{ContinuedFloat}{0}
\setcounter{KVtest}{0}
\setcounter{subfigure}{0}
\setcounter{subfigure@save}{0}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{subtable@save}{0}
\setcounter{lotdepth}{1}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{Item}{16}
\setcounter{Hfootnote}{29}
\setcounter{bookmark@seq@number}{43}
\setcounter{ALC@unique}{0}
\setcounter{ALC@line}{0}
\setcounter{ALC@rem}{0}
\setcounter{ALC@depth}{0}
\setcounter{algorithm}{0}
\setcounter{section@level}{3}
}
