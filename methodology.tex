\chapter{Methodology}
\label{chap:methodology}

 \hypersetup{
	colorlinks=false,
	linkcolor=black,
	filecolor=black,
	citecolor = blue,      
	urlcolor=cyan,
}

In this chapter, the methodology used to implement the system is illustrated. We start with an overview of the proposed system. An interpretation of the choice of architecture and the decisions made is also elaborated. Eventually, the system implementation steps are demonstrated in detail, along with all attempts and procedures.

\includefig{0.4}{my_system}{Automated Dispatcher Actions System comprising two sub-systems: Automatic Speech Recognition unit taking input as raw speech from the driver. The text output is fed into a trained Text Classifier which issues the proper action accordingly.}{Fig:10}



\section{System Overview} 
\label{meth:s1}

As discussed earlier, the main purpose of this study is to implement a system able to issue dispatcher actions automatically when provided with audio signals from the vehicle driver, hence, introducing more automaton in the control-center. 

Our system consists of two sub-systems, as shown in figure \ref{Fig:10}, the first sub-system is the \ac{ASR} unit, which takes as input raw speech waveforms produced by the driver, and produces the information in a text form. The text is then passed to a trained text classifier, which given the text information from the \ac{ASR}, issues the corresponding appropriate dispatcher action.

TODO IN THE CHOICE OF ARCHITECTURE NLP PROBLEMS TO MAKE AN END TO END SYSTEM

\section{Automatic Speech Recognition Unit} 
\label{meth:s2}

One of the most crucial choices made was the decision about the \ac{ASR} system since high quality \ac{ASR} is a chief requirement for speech-based applications. We began by searching for open-source free German \ac{ASR}s with acceptable performance. Though many freely available state-of-the-art \ac{ASR} systems were found, they were mostly English \ac{ASR}s; this is mainly due to the availability of open-source English training data and the lack of other languages data available for free. Despite being rare, we found an interesting open-source German speech recognition model \cite{milde2018open} with freely available training recipes, open source training data, and pre-trained models ready for download and use. This model is based on Kaldi toolkit \cite{daniel2011kaldi} which is an open-source toolkit for speech recognition research. This model \cite{milde2018open} uses \ac{GMM} - \acf{HMM} and \ac{TDNN} \cite{waibel1990readings} \cite{peddinti2015time}, following the TED-LIUM corpus recipe example \cite{rousseau2014enhancing} in Kaldi. The model \cite{milde2018open} was trained on the German subset of the \ac{SWC} dataset\footnote{\url{https://en.wikipedia.org/wiki/Wikipedia: WikiProject_Spoken_Wikipedia}} (285 hours), Tuda-De dataset\footnote{\url{http://speech.tools/kaldi_tuda_de/german-speechdata-package-v2.tar.gz}} \cite{radeck2015open} (160 hours) and M-AILABS dataset\footnote{\url{https://www.caito.de/2019/01/the-m-ailabs-speech-dataset/}} (237 hours). For now, we only indicate the number of hours for each dataset, however, we shall discuss these datasets in details in section SECTION. The model therefore was trained on a total of 630 hours, and achieved $14.78\%$ \ac{WER} on the dev set of Tuda-De (11.5 hours) and $15.87\%$ on the test set of it (11.9 hours).


The above model \cite{milde2018open} is referred to as \enquote{Hybrid Model}, making use of both \ac{HMM}s and neural networks. We found no open-source end-to-end German \ac{ASR}, and despite the discussed model \cite{milde2018open} having satisfying performance, the idea of an end-to-end German \ac{ASR} was more attractive. This is because end-to-end systems have a much simpler training procedure, as many hand-engineered components and sophisticated pipelines are replaced by neural networks. End-to-end speech recognition is a fast-paced developing area of study with results improving continually. The end-to-end vision liberates us from domain-specific knowledge required for alignment, \ac{HMM} design, etc. And since end-to-end \ac{ASR} systems require little task specific components, an end-to-end English \ac{ASR} can be easily adapted to another language. A further advantage for end-to-end \ac{ASR}s is that they are more robust to noise and variation in speech and requires no special handling for them. In light of the great advantages for end-to-end systems, it was decided to find a suitable end-to-end \ac{ASR} that could be adapted to German without much complexity, and to use the Kaldi based model \cite{milde2018open} for comparative purposes only.



One seemingly convenient model was Deep Speech 2 \cite{amodei2016deep}, demonstrated in section \ref{bg:sub8}. The authors of the paper used their model for both English and Mandarin, two widely different languages. We took that as a proof of concept that the model could be easily adapted with very little changes to other languages including German. We started by mining for open source freely available German speech datasets that could be used since end-to-end \ac{ASR} systems require large amounts of transcribed audio data. We list the datasets found, their description, problems encountered while dealing with them and the cleaning procedures.


\subsection{Speech Recognition Datasets}
\label{meth:sub1}

For the purpose of unification, all

\subsubsection{\RomanNumeralCaps{1}. Common Voice}
\label{meth:subsubsub1}

Common Voice is the largest open source, multi-language dataset of voices available for use. It is managed by \href{https://www.mozilla.org/en-US/}{Mozilla} and was collected by volunteers on-line who were either recording samples of audio or validating other samples. Mozilla began work on this project in 2017 and contribution to the dataset continues up till now. Since most of the data used by large companies isn't available freely to researchers, Mozilla's aim was to create a free public dataset to make speech recognition open and accessible to everyone.
For our \ac{ASR}, the German subset of the dataset was selected. It incorporates 340 total hours, with 325 validated hours and 15 invalidated hours which were excluded. The dataset has 5007 speakers but as we discarded the invalidated utterances we end up with only 4823 speakers. 
All the utterances were in \enquote{mp3} format and sampled using a sampling rate of $44 kHz$ so we converted them to \enquote{wav} format and we performed down-sampling to obtain sample rate of $16 kHz$. We checked for any corrupted files but there were none.


\subsubsection{\RomanNumeralCaps{2}. M-AILABS Speech Dataset}
\label{meth:subsub2}
M-AILABS Speech Dataset is an open-source multi-lingual dataset provided by Munich Artificial Intelligence Laboratories GmbH. Most of the data is based on LibriVox \footnote{\url{https://librivox.org}} and Project Gutenberg \footnote{\url{https://www.gutenberg.org}}. We make use of the German subset which is 237 hours 22 minutes with a total of 5 speakers. The data is available in \enquote{wav} format and sample rate of $16 kHz$ so we perform no modifications. We also check for corrupted files but all of them were healthy.

\subsubsection{\RomanNumeralCaps{3}. German Speech Data \cite{radeck2015open}}
\label{meth:subsub3}

This open-source corpus is provided by Technische Universit{\"a}t Darmstadt. It has 36 hours read by 180 speakers, and recorded using 5 different microphones simultaneously. They made use of the KisRecord \footnote{\url{http://kisrecord.sourceforge.net}} toolkit, which allows for recording with multiple microphones concurrently. Their target was distant speech recognition, thus a distance of one meter between speakers and microphones was chosen. The sentences which volunteers were provided to read were extracted randomly from three text resources: German Wikipedia, German section of the European Parliament transcriptions, short commands for command-and-control settings. Unfortunately, there were some corrupted files in the dataset, which we discarded.

\subsubsection{\RomanNumeralCaps{4}. German Single Speaker Data}
\label{meth:subsub6}
hi

\subsubsection{\RomanNumeralCaps{3}. Movies Data}
\label{meth:subsub7}
hi

\subsubsection{Dataset Splitiing: Test, Dev, Train}
\label{meth:subsub7}
TODO SPEAKER INDEPENDENT CLASSIFICATION

\begin{table}[!ht]
	\centering
	\begin{tabular}{ | c | c | c | c | c | } 
		\hline
		&   Train   &   Dev &   Test    &   Speakers \\ 
		\hline
		Tuda-De &   160.15h &   11.53h  &   11.9h   &   179 \\
		\hline
		M-AILAB &   233.71h &   -   &   -   &   5+mixed \\ 
		\hline
		Common Voice    &   274.75h &   23.1h   &   20.34h  &   4823 \\
		\hline
		Total   &   668.61h &   34.63h  &   32.24h  &   5007 \\
		\hline
	\end{tabular}
	\caption{First 14 Epochs Data}
	\label{table:data14}
\end{table}

\begin{table}[!ht]
	\centering
	\begin{tabular}{|c|c|c|}
		\hline
		Epoch & WER    & CER    \\ \hline
		1     & 61.286 & 16.690 \\ \hline
		2     & 46.4 & 11.998 \\ \hline
		3     & 40.663 & 10.190 \\ \hline
		4     & 34.519 & 8.456  \\ \hline
		5     & 32.162 & 7.801  \\ \hline
		6     & 29.811 & 7.368  \\ \hline
		7     & 28.487 & 6.903  \\ \hline
		8     & 27.907 & 6.927  \\ \hline
		9     & 26.624 & 6.543  \\ \hline
		10    & 27.005 & 6.662  \\ \hline
		11    & 26.004 & 6.413  \\ \hline
		12    & 25.658 & 6.320  \\ \hline
		13    & 25.499 & 6.282  \\ \hline
		14    & 25.802 & 6.350  \\ \hline
	\end{tabular}
	\caption{}
	\label{table:train14}
\end{table}

\begin{table}
\centering
\begin{tabular}{ | c | c | c | c | c | } 
	\hline
	& Train   & Dev    & Test   & Speakers  \\ 
	\hline
	Tuda-De      & 160.15h & 11.53h & 11.9h  & 179       \\
	\hline
	M-AILAB      & 233.71h & -      &   -    & 5+mixed   \\ 
	\hline
	Common Voice & 274.75h & 23.1h  & 20.34h & 4823      \\
	\hline
	Movies       & 42h     & -      & -      & -         \\
	\hline
	CSS10        & 16.7    & -      & -      & 1         \\
	\hline
	Total        & 727.31h & 34.63h & 32.24h & >5008     \\
	\hline
\end{tabular}
\caption{After adding Movies and CSS10 data}
\label{table:data19}
\end{table}


Many open source implementations for the model were found that could be used such as \footnote{\url{https://github.com/noahchalifour/baidu-deepspeech2}} \footnote{\url{https://github.com/SeanNaren/deepspeech.pytorch}} \footnote{\url{https://github.com/tensorflow/models/tree/master/research/deep_speech}}. On the first attempt, we decided to use the \href{https://www.tensorflow.org/}{TensorFlow} \cite{tensorflow2015-whitepaper} implementation available in the TensorFlow Github repository \url{https://github.com/tensorflow/models/tree/master/research/deep_speech}. TensorFlow is an open source library for numerical computation used for machine applications. It was developed and maintained by \href{https://ai.google/research/teams/brain}{Google Brain}. At the beginning it was intended for internal use only, however, on  November 9, 2015, it was released under the \href{http://www.apache.org/licenses/LICENSE-2.0}{Apache License 2.0}.

One of the earliest problems encountered was our inability to run the model locally, as it is a large model demanding very high computational resources. As a result, we turned to \href{https://colab.research.google.com/}{Google Colab}, which is a free cloud platform used for machine learning education and research. It offers researchers the chance to run their applications using either a free GPU, or a cloud \ac{TPU}. A \ac{TPU} is an \ac{ASIC} optimized for performing high speed addition and multiplication operations, it was developed by Google for the purpose of speeding up machine learning applications. The implementation used TensorFlow \href{https://www.tensorflow.org/guide/estimators}{Estimators}, which is a high-level TensorFlow API that facilitates machine learning programming. Estimators encapsulate training, evaluation and prediction. They provide a training loop that controls building the graph, initializing variables, loading data, handling exceptions and creating checkpoints. For using Estimators, the data input pipeline should be separated from the model, thus making the model easier to work with different datasets. There are two types of Estimators: regular estimators, which work for both GPUs and CPUs, and \href{URL}{TPU Estimators}, which work for Google's cloud \ac{TPU}s. Since the implementation used normal estimators, we decided to run on Google Colab using a GPU which was a Tesla K80 with 12 GB memory.


As an initial experiment to make sure everything works, we used only the Tuda-De data, and used a model of 5 layers and 800 hidden size. After few days, the training reached for 3 epochs and a \ac{WER} of $80\%$. The issue however was clear, it was infeasible to proceed. The training was extremely time consuming and it would take months for the model to converge. We tried to make the model's size smaller and used 3 layers with hidden size of 700. With this setting and using Tuda-De only, the model converged at $50\%$ \ac{WER} after more than a week. We made sure the model was working and able to learn, however, the training speed was considerably slow and it was infeasible to train on the whole datasets we collected.


In search of other alternatives, we tried to make use of \href{https://www.tensorflow.org/tfrc}{TensorFlow Research Cloud Program}. This program offers researchers free cloud \ac{TPU}s in order to run their machine learning applications. We were offered $5$ v2 cloud \ac{TPU}s, each \ac{TPU} had $8$ cores with each core having $8$ GiB of \ac{HBM}. The \ac{TPU}s were available for use on \href{https://cloud.google.com/}{Google Cloud Platform}. The major obstacle was that the implementation used regular estimators; to overcome this it was necessary to migrate from normal estimators to \ac{TPU} estimators. This process was a simple one that required little code changes. The main issue, however, was that models are compiled using \href{https://www.tensorflow.org/xla/}{\ac{XLA}} when using \ac{TPU}s. \ac{XLA} is a compiler that requires that tensor dimensions must be statically defined at compile time. The problem was that not all utterances had the same number of frames, and we could not set a specific length to pad up to it and truncate smaller than it. The problem with truncating is that if we truncate after $n$ frames, how do we determine the labels to discard after the $n^{th}$ frame? The only solution was to pad the frames with zeros up to the maximum sequence length in the dataset, in this case, Tuda-De. After padding to the maximum number of frames, the \ac{TPU}s ran out of memory. After searching for solutions to this problem, the only solution found was to upgrade to TPU-v3 which 16 GiB of \ac{HBM} for each \ac{TPU} core, or to use a \ac{TPU} pod, which is a multiple \ac{TPU} devices connected together and the workloads distributed across all devices. Both options were infeasible for us.


In searching for other options, it was finally decided to abandon the TensorFlow implementation due to the problem of the \ac{TPU}s requiring tensors of static dimensions. We tried to use the PyTorch implementation of the model \url{https://github.com/SeanNaren/deepspeech.pytorch}. 
\href{https://pytorch.org/}{PyTorch} is an open source machine learning library which was developed by 
Facebook's artificial intelligence research group. It is based on Torch library and was released under the \href{https://en.wikipedia.org/wiki/BSD_licenses#3-clause}{ Modified BSD license}. In order to make sure the model was working and able to learn, we also used only Tuda-De data, with model of 3 layers and hidden size of 700 nodes. The training was started on Google Colab using a Tesla K80 GPU with 12 GB memory. The model was gradually improving, however it was also considerably slow.

At this point it was obvious there is a computational resources problem; Colab was very slow to use as it only used one GPU, and using the TPUs was not attainable. A cloud platform was needed in order to run the model and be able to iterate and experiment faster. For this purpose, we made us of Microsoft Azure's NC-series virtual machines. NC-series \ac{VM}s are armed with NVIDIA Tesla K80 GPUs and Intel Xeon E5-2690 v3 (Haswell) processors.
We chose Standard NC12 size, with 12 CPUs and a total of 112 GiB memory. It has also 2 GPUs with GPU memory of 24 GiB and SSD storage of 680 GiB.
All of our previous experiments were using \ac{GRU}s as the hidden unit of the \ac{RNN} layers. We decided to run two instances of training: one using 3 layers, 700 nodes, and \ac{GRU}s. The other one using 3 layers, 700 nodes and \ac{LSTM}s. After few epochs, the \ac{LSTM} instance was performing much better, however we did not proceed with training. At this point, we made sure the model was working and we got acceptable computational resources to run the training. In order to be able to use all of the datasets for the training, we upgraded the \ac{VM} to Standard NC24. This type has 24 CPUs and a total of 224 GiB memory, 4 GPUs with GPU memory of 48 GiB and SSD storage of 1440 GiB.


\subsubsection{problem of adding swc}
We were faced with a problem when trying to use the German \ac{SWC} dataset. The utterances were whole articles of very long durations that the memory would not fit them, even with the batch size set to 1. The dataset came with a segmentation file that included the alignments for the sentences. We used this segmentation file to cut the long wav files into small ones, however, after manually analyzing a random sample of the generated files, a high percentage of it was found to be very faulty. Since we have no method of aligning the data ourselves like in \cite{amodei2016deep}, we refrained from using the \ac{SWC} as we suspected it would deteriorate the overall performance.

We started the training with model size 5 layers, 800 nodes, and we added all the datasets except \ac{SWC}, using \ac{LSTM} hidden units. The training procedure took 10 days, through which we ran 19 epochs. It should be noted that after epoch 13, we used the noise augmentation option available and implemented in the pytorch model. It applies some changes to the tempo and gain when it's loading the audio. This aims at increasing robustness to environmental noise.

\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|}
	\hline
	Epoch & WER    & CER    \\ \hline
	1     & 61.286 & 16.690 \\ \hline
	2     & 46.4 & 11.998 \\ \hline
	3     & 40.663 & 10.190 \\ \hline
	4     & 34.519 & 8.456  \\ \hline
	5     & 32.162 & 7.801  \\ \hline
	6     & 29.811 & 7.368  \\ \hline
	7     & 28.487 & 6.903  \\ \hline
	8     & 27.907 & 6.927  \\ \hline
	9     & 26.624 & 6.543  \\ \hline
	10    & 27.005 & 6.662  \\ \hline
	11    & 26.004 & 6.413  \\ \hline
	12    & 25.658 & 6.320  \\ \hline
	13    & 25.499 & 6.282  \\ \hline
	14    & 24.956 & 6.132  \\ \hline
	15    & 24.554 & 6.032  \\ \hline
	16    & 23.290 & 5.703  \\ \hline
	17    & 23.610 & 5.840  \\ \hline
	18    & 22.919 & 5.605  \\ \hline
	19    & 22.875 & 5.629  \\ \hline
\end{tabular}
\caption{Summary of the training}
\label{table:train19}
\end{table}

\includefig{0.9}{training}{Training Summary}{Fig:20}

\includefig{0.9}{trainingep19}{Training Summary}{Fig:21}


After 19 epochs, the model converged at \ac{WER} 29.571 and \ac{CER} 7.844 on our test set.  The fact that ctc adopt the assumption that the output labels are conditionally independent often makes the outputs suffer from errors that needs lingustic information to be corrected. In attempts to achieve better results, three techniques were investigated, and we discuss them in the following two subsections:

\subsubsection{Language Model Decoding}

In speech recognition, it is convenient to couple the model with a language model decoding technique in hope for achieving better \ac{WER}. Since there is more text data available more than transcribed audio, we can train a language model on massive amount of text corpora to generate a powerful n-gram language model. For this task, we make use of \href{https://kheafield.com/code/kenlm/}{KenLM}, which is a Language Model Toolkit that is easy to use and can be used to generate n-grams language models of any order. The target was to collect abundant text corpora to experiment with language models. The datasets we found and used are listed here:

\begin{enumerate}
	\item \textbf{German Wikipedia} \\
	We made use of the available \href{https://dumps.wikimedia.org/dewiki/latest/}{German Wikipedia dump} available for free download. The wikipedia was downloaded as one 20 GB xml file. We used a WikiExtractor library\footnote{\url{https://github.com/attardi/wikiextractor}} to extract the wikipedia into text files, which gave us a total of 56 folders, each containing 100 text files. Since KenLM requires that text data be in one file, with one sentence per line, we used spaCy\footnote{\url{https://spacy.io/}} which is a Python library for \ac{NLP} tasks. It supports many languages including German, we make use of the \href{https://spacy.io/models/de}{German model} and run it on all of the wikipedia text files. In order for the corpus to match the output of the \ac{ASR}, we perform the same text cleaning performed on the \ac{ASR} transcriptions: lowering all the letters, replacing numbers with their written form \textit{i.e.} 139 is changed into einhundertfUnfunddreissig, removing all punctuations and limiting all the vocab to letters from a to z plus the German umlauts A O U. All the B was replaced with ss and all the foreign characters were stripped out as well.
	
	
	
	\item \textbf{5 Million Web Sentences} \\
	We also make use of the Leipzig Corpora Collection offered by Universitat Leipzig. We make use of the data available in web section, and collect a total of 5Million web sentences. The data was found to be one sentence per line and need not further processing other than the same text cleaning performed on the Wikipedia.
	
	
	\item \textbf{8 Million Mary Sentences} \\
	We make use of the data used for language modeling by \ref{milde2018open}, available for download here\footnote{\url{ http://speech.tools/kaldi_tuda_de/German_sentences_8mil_filtered_maryfied.txt.gz}}. The data was in the needed format, with one sentence per line and needed no further processing.
	
	
	\item \textbf{Our transcriptions} \\
	We also make use of the transcriptions for our collected audio datasets. It was already one sentence per line and cleaned and suitable for use without any further processing.

	
\end{enumerate}

All of our data was augmented in one text file. We started by installing KenLM dependencies, and then installing KenLM itself. We perform word tokenization on our data using \href{https://www.nltk.org/}{nltk} library, then train an n-gram model with Kneser-Ney smoothing using KenLM. The generated output is a .arpa file which has a data section with unigram, bigram,..., n-gram counts followed by the estimated values. Though the .arpa format is more readable, the .binary format is much faster and more flexible. It remarkably reduces the loading time and the Language model is in fact required to be in .binary format to be used with our model. For these reasons, we binarize the model and obtain the final .binary labguage model file used for decoding. It should be noted that language model decoding with beam search is given by equation \ref{eq:25}

\begin{equation}
Q(y) = \log(p_{RNN}(y|x)) + \alpha \log(p_{LM}(y)) + \beta wc(y)
\end{equation}

where $wc(y)$ is the number of words in the transcription y. The weight $\alpha$ favors the outputs of the language model, while the weight $\beta$ encourages more words in the transcription.

We experiment with many data combinations and different values for $n$ which we report in details in the results section, however, our best result was $15.587\%$ \ac{WER} and $5.806\%$ \ac{CER} when using all our text corpora, with a 5-gram model and beam-with value of 500. The alpha and beta values were set to 0.9 and 0.2 respectively.

\subsubsection{Language Model Rescoring}

It is common as well to arm the model with language model rescoring technique which performs recognition in two passes. In the first pass, a relatively small language model with a low value of $n$ is used \textit{i.e.} 3 or 4 gram; this often makes the decoding process much simpler. We use N-best methode where the N-best scoring hypotheses outputs from the small language model are re-scored using a larger language model of higher $n$ value \textit{i.e.} 5 or 6 gram. The best scoring hypothesis according to the larger language model is then chosen to be the predicted transcription.
We expriment with various values for $n$ for the smaller language model and the larger one. The different settings with their respective results are reported in details in the results section \ref{chap:results}, howver, our best \ac{WER} was $x \%$ and was achieved with a small language model of $x$-gram built from all our text corpora, and a large language model of $x$-gram built as well from all the text corpora. We used a beam-width of $x$ value, and we set alpha to 0.9, and beta to 0.2.


\subsubsection{Auto Correct With Transformer}

In this experiment, we employ the method suggested in \cite{zhang2019automatic} by implementing an auto correct model that automatically corrects errored outputs of the ctc-based network. The main idea is to train a model which takes as input at inference the predictions generated by the \ac{ASR} model, and outputs the correct transcriptions. We stick to the model proposed in the paper \cite{zhang2019automatic} which is the Transformer \cite{vaswani2017attention}, however, we use the Transformer model implemented in \href{https://github.com/tensorflow/tensor2tensor}{tensor2tensor} library. \ac{Tensor2Tensor} is a machine learning library developed and maintained by \href{https://ai.google/research/teams/brain}{Google Brain} that aims at making machine learning research accessible and easier for researchers.
We start off by installing the library and then turn to defining our problem. The library's general interface is to define a problem that can be solved using different models and/or hyper-parameters. There are pre-defined problems in the library, however, we define our own and we call asr\_correction problem. Our problem is very similar to the translation problems defined in the library. We experiment with different datasets and hyper-parameters. For the training data, we use the movies data REFERENCE SECTION and the single speaker data REFERENCE as these two datasets were only added after epoch 13 in the asr, hence we can use the model's output at epoch 13 to transcribe these utterances. The training examples are made by pairing each transcribed output from the \ac{ASR} with its ground truth. When transcribing these utterances using greedy decoding, we obtain a total of 75K training examples.  For the sake of increasing the number of training examples, we use a $x$-gram model with beam-width $x$, 0.9 alpha and 0.2 beta, and get the best 10 outputs, and pair these outputs with the corresponding ground truth. This technique increases our training examples by a factor of 10 resulting in 750K training examples. We experiment with two hyperparameter sets of the transformer: transformer\_base\_single\_gpu and transformer\_big. We train both instances for $x$ steps, using the 750K examples. The transformer\_big achieved $x\%$ \ac{WER}, and $x\%$ \ac{CER}, while the transformer\_base\_single\_gpu  achieved $x\%$ \ac{WER}, and $x\%$ \ac{CER}. We also train the transformer\_big using the 75K training examples, however, more data got us an increase in performance by $x\%$.



%The CTC loss function (Graves et al., 2006) coupled with an RNN to model temporal information also performs well in endto-end speech recognition with character outputs (


%Due to the large amount of speakers and the diversity
%of topics included in the training data, our model is robust
%against speaker variation and topic shift.

For the purpose of comparing our system with the hybrid kaldi-based model \cite{milde2018open} illustrated in SECTION. We attempt to test the kaldi-based model on our test-set. We start by installing kaldi, and this \href{https://github.com/alumae/kaldi-gstreamer-server}{Kaldi GStreamer server} which is a real-time full-duplex speech recognition server. It is implemented in Python and based on GStreamer framework and Kaldi toolkit. Our test set is transcribed using the Kaldi GStreamer server and Kaldi's best model\footnote{\url{http://ltdata1.informatik.uni-hamburg.de/kaldi_tuda_de/de_400k_nnet3chain_tdnn1f_2048_sp_bi.tar.bz2}}. Then the output transcriptions are compared with the ground truth transcriptions in order to compute the \ac{WER} and the \ac{CER}. After comparing the transcriptions, we got $x\%$ \ac{WER} and  $x\%$ \ac{CER} on our dataset. This means that our best model outperforms the kaldi-based model with $x\%$ improvement in \ac{WER} and $x\%$ improvement in \ac{CER}

We also noticed that Kaldi-based model was tested on Tuda-De dataset excluding the realteck microphone since it's very low quality. We excluded all utterances recorded using this microphone, and tested our best model on the modified Tuda-De's test set. Our best model was after 19 epochs of training and coupled with a 5-gram language model decoding technique, beam-width value of x, using alpha and beta values of 0.9 and 0.2 respectively. We achieved $x\%$ \ac{WER} and $x\%$ \ac{CER} outperforming the kaldi-based model on this test set as well.

LEARNING RATE
BATCH SIZE 20
SAMPLE RATE 16KHZ
CHANGED LABELS TO A-Z PLUS UMLAUTS

\section{Text Classifier Unit} 
\label{meth:s3}

In this section, we discuss the second part of our proposed system: the text classifier unit. The model for the text classifier was chosen to be Google's \acf{BERT} \cite{devlin2018bert}, illustrated in section \ref{bg:sub13} of the literature review. We illustrate the underlying theory behind our decision for using \ac{BERT} over a large variety of both simple classifiers and general language-understanding models.

\subsection{Why BERT?}

 A strong point of \ac{BERT} over simple text classifiers is that we can make use of large amounts of unlabeled data to compensate for small labeled task-specific datasets. This is specially useful when we do not know details about the task-specific dataset, at least for the time being. (\textit{i.e.} its size, its format, etc.) When using \ac{BERT} as a classifier on \ac{MRPC}\footnote{\url{https://www.microsoft.com/en-us/download/details.aspx?id=52398}}, which contains only $3,600$ training examples, the results were ranging between $84\%$ and $88\%$. These results are very satisfying for such a small dataset. Since we do know the amount of data available for our task and given the results published for the \ac{MRPC} dataset classification task, we assume that \ac{BERT} would be of great help if the data available is of relatively small size. Moreover, we do not know the nature of the data available for our task. Using \ac{BERT}, we can model our problem as a classification task, a question answering task, etc. Though we make the assumption that our problem is a text classification one, we utilize \ac{BERT} for more flexibility. A further argument is the great importance for our task to get contextual information instead of performing shallow classification or using non-contextual learned embeddings or even using a model that shallowly concatenate left and right context. As discussed in section \ref{bg:sub13}, \ac{BERT} is a deep bidirectional model since it uses \acf{MLM} as one of its pre-training tasks. The next sentence prediction task also makes it model relationships between sentences, which is important for our task as well. In addition to this, fine-tuning a classifier on top of \ac{BERT} is a fairly simple task. Google published two versions of pre-trained \ac{BERT}, an English model, and a multilingual model that could be used for 104 languages, since our language of interest is German only, we suggest that training a German only model would be much more powerful for our intended purpose.

\subsection{Implementation}
Once we settled on using \ac{BERT} for our text classifier unit, came the step of finding an implementation. For the sake of saving time and effort, we refrain from implementing our own model and search for a convenient implementation. Though there are many freely available implementations for Bert, we stick to the official Google implementation available in \url{https://github.com/google-research/bert} as it provides more flexibility. The implementation is based on \href{https://www.tensorflow.org/}{TensorFlow} library and makes use of the \href{https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator}{Estimator API}. The open source code offers scripts for creating the training data, pre-training and finetuning, which minimizes the amount of work and modifications needed. As demonstrated earlier in the literature review, using \ac{BERT} consists of two stages: pre-training and fine-tuning. We discuss the steps pursued during both stages.

\subsection{Pre-training Data}
For pre-training \ac{BERT}, we needed large amount of unlabeled text data. The published \href{https://github.com/google-research/bert#pre-trained-models}{pre-trained English models} were trained on both the Book Corpus\footnote{\url{https://yknzhu.wixsite.com/mbweb}} (800M words), and the English Wikipedia\footnote{\url{https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2}} (2.5B words). The Book Corpus is not available for free download and we do now if it contains any German data, therefore we abstain from using it. Since we are interested in German, we make use of the German Wikipedia\footnote{\url{https://dumps.wikimedia.org/dewiki/latest/dewiki-latest-pages-articles.xml.bz2}} alone (700M words - 52M sentences). It is convenient for use since the articles nature fits the next sentence task since most sentences are correlated. We considered using the sentences corpus provided by Universit{\"a}t Leipzig\footnote{\url{http://wortschatz.uni-leipzig.de/en/download}}, however they were separate sentences and would not add much to the next sentence prediction task. 

\subsubsection{Pre-processing German Wikipedia}
We start by downloading the \href{https://dumps.wikimedia.org/dewiki/latest/dewiki-latest-pages-articles.xml.bz2}{German Wikipedia dump}. The whole wikipedia is downloaded as one 20GB file in \texttt{xml} format. We use an open source wikiExtractor\footnote{\url{https://github.com/attardi/wikiextractor}} in order to extract it into plain text files. The Wikipedia is extracted into $5,600$ text files split upon $56$ folders, each $100$ files in a folder. Since the input to our subsystem would be the output from the \ac{ASR} unit, we perform some text cleaning in order to match the output of the \ac{ASR}. The \ac{ASR} outputs only lower-case letters (\enquote{a} to \enquote{z}), apostrophe ('), space, plus the German umlauts ({\"a} {\"o} {\"u}). For this purpose, we replace all numbers with their written format \textit{i.e.} 37 is turned into \enquote{siebenunddreissig}. Any \enquote{\ss} was replaced with \enquote{ss}. All punctuation and foreign characters were stripped away, and only the allowed characters \textit{i.e.} potential outputs of the \ac{ASR} were kept in the corpus.


The implementation requires the input data to be in specific format: each file has to be a plain text file, with one sentence per line. Different documents must be delimited by empty new lines; this is essential for the next sentence prediction task.
To get the data in the proper format, we use the spaCy\footnote{\url{https://spacy.io/}} toolkit, which is a Python library for \ac{NLP} tasks. It supports many languages including German, we make use of the \href{https://spacy.io/models/de}{German model} and run it on all of the Wikipedia text files. There were many possible choices for the sentence tokenization task such as nltk\footnote{\url{https://www.nltk.org/}}, OpenNLP\footnote{\url{https://github.com/apache/opennlp}}, etc. Nevertheless, we stick to spaCy since it is recommended by \ac{BERT}'s official \href{https://github.com/google-research/bert}{github repository}. The sentence tokenization is not flawless and by manually analyzing random samples of the files, it is found that many errors exist. We ignore the mistakes albeit and consider it as noise in the input data as it is advised in the repository to add a slight amount of noise to the input data. This is particularly beneficial to make the model robust to slightly different data during fine-tuning step. In addition, distinct articles are separated with empty lines.

\subsubsection{Generating Vocab File}
As previously illustrated, \ac{BERT} uses word pieces instead of whole words. The process is done as follows: a vocab file containing sub-words is generated from the whole the pre-training data. This list of sub-words (referred to as vocab file), is used to tokenize the pre-training data and the fine-tuning data. Out of vocab sub-words are given a special \texttt{[UNK]} token. The official repository does not include code for learning new vocab, however, they refer to multiple alternatives such as Google's SentencePiece library\footnote{\url{https://github.com/google/sentencepiece}} and tensor2tensor's WordPiece generation script\footnote{\url{https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/text_encoder_build_subword.py}}. Instead, we use an open source bert-vocab-builder\footnote{\url{https://github.com/kwonmha/bert-vocab-builder}}, which is a modification to tensor2tensor's \href{https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/text_encoder_build_subword.py}{WordPiece generation script}. The modifications introduced aims at making the generated vocab list more compatible with the tokenization script provided in \ac{BERT}'s repository. Changes include adding \ac{BERT}'s special tokens (\texttt{[SEP]}, \texttt{[CLS]}, \texttt{[MASK]}, \texttt{[UNK]}) to the vocab list, and denoting split words using \enquote{\texttt{\#\#}} instead of \enquote{\texttt{\_}}. 

A threshold of 800 is used in selecting the sub-words. \textit{i.e.} any sub-word occurring in the training corpus less than 800 times will be excluded from the vocab list. Using this threshold, we end up with a vocab list of size	$\approx 10,000$ sub-words. In later stages, all our data will be tokenized according to this vocab list.


\subsubsection{Tokenization and Writing into TFRecord Format}

After generating the vocab file, the pre-training corpus has to tokenized according to the produced vocab file. Google provided scripts for both tokenizing the corpus given a vocab file and creating the training data in the \enquote{TFRecord} format. TFRecord is a simple binary file format that many TensorFlow applications use for training data. In order to write the data into TfRecord format, the data has to be converted into sequence of byte-strings. Instead of concatenating all the Wikipedia's $5,600$ text files into one file, we modify the script to read the input files from a \texttt{.csv} file containing the paths for all the cleaned text files. This is done because all the examples of the input file will be stored in memory, so for the purpose of avoiding out of memory issues, we keep the files sharded and we modify the code to generate one TFRecord file for each $100$ text files \textit{i.e.} $56$ total TFRecord files. 
A \enquote{\texttt{max\_seq\_length}} 
is set when creating the data, where sequences of lengths longer than this value are truncated and sequences of shorter lengths are padded. We choose this value to be $128$. Another values set are the \enquote{\texttt{max\_predictions\_per\_seq}} and \enquote{\texttt{masked\_lm\_prob}}. The \texttt{masked\_lm\_prob} is the probability of masked tokens, we set this value to 0.15 \textit{i.e.} $15\%$. The \texttt{max\_predictions\_per\_seq} is the maximum number of \ac{MLM} predictions per sequence. This value should be set to be \texttt{max\_seq\_length} * \texttt{masked\_lm\_prob}, therefore we set it to $20$. It is to be noted that the script does not get this value automatically because this value has to be passed to both the data creation script and the pre-training script.
 
 
\subsection{Pre-Training Process}

For pre-training, we set the \texttt{batch\_size} to 896 (largest that could fit into \ac{TPU} memory). We set the \texttt{max\_seq\_length} and \texttt{max\_predictions\_per\_seq} to $128$ and $20$ respectively since these values were used when creating the data. We train for $90,000$ steps and set the \texttt{num\_warmup\_steps} to $10,000$ steps. The number of warmup steps EXPLAIN. We set the initial \texttt{learning\_rate} to $1e-4$.

The \texttt{num\_hidden\_layers} and \texttt{num\_attention\_heads} are both set to 12, the \texttt{vocab\_size} is set to 105944. The \texttt{hidden\_act} which is thw activation function used is the gelu activation function. 
In the \texttt{ber\_config\_file}, we specify some configurations such as the number 

TFRC program, tpu versions
 
-platform to run google cloud, tensorflow research, tpu specs

-training steps, hyperparameters, bert config file
The model configuration (including vocab size) is specified in bert config file

%	"directionality": "bidi"
%	"hidden\_dropout_prob": 0.1, 
%	"hidden\_size": 768, 


%The learning rate we used in the paper was 1e-4. However, if you are doing additional steps of pre-training starting from an existing BERT checkpoint, you should use a smaller learning rate (e.g., 2e-5).

%Longer sequences are disproportionately expensive because attention is quadratic to the sequence length. In other words, a batch of 64 sequences of length 512 is much more expensive than a batch of 256 sequences of length 128. The fully-connected/convolutional cost is the same, but the attention cost is far greater for the 512-length sequences. Therefore, one good recipe is to pre-train for, say, 90,000 steps with a sequence length of 128 and then for 10,000 additional steps with a sequence length of 512. The very long sequences are mostly needed to learn positional embeddings, which can be learned fairly quickly. Note that this does require generating the data twice with different values of max_seq_length.

%recommended recipe is to pre-train a BERT-Base on a single preemptible Cloud TPU v2, which takes about 2 weeks at a cost of about 500 USD (based on the pricing in October 2018). You will have to scale down the batch size when only training on a single Cloud TPU, compared to what was used in the paper. It is recommended to use the largest batch size that fits into TPU memory.

 Uncased model to match output of the asr
 

\subsection{Fine-Tuning Data}
 -finetuning: 10K articles
 
\subsection{Fine-Tuning Process}
 finetuning steps, hyperparameters,
 creating a classification layer

-effect f running pre-training on finetuning data

%\subsubsection{\RomanNumeralCaps{2}. 10K German Articles}
%\label{meth:subsub5}

