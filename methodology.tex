\chapter{Methodology}
\label{chap:methodology}

 \hypersetup{
	colorlinks=false,
	linkcolor=black,
	filecolor=black,
	citecolor = blue,      
	urlcolor=cyan,
}

In this chapter, the methodology used to implement the system is illustrated. We start with an overview of the proposed system. An interpretation of the choice of architecture and the decisions made is also elaborated. Eventually, the system implementation steps are demonstrated in detail, along with all attempts and procedures.

\includefig{0.4}{my_system}{Automated Dispatcher Actions System comprising two sub-systems: Automatic Speech Recognition unit taking input as raw speech from the driver. The text output is fed into a trained Text Classifier which issues the proper action accordingly.}{Fig:10}



\section{System Overview} 
\label{meth:s1}

As discussed earlier, the main purpose of this study is to implement a system able to issue dispatcher actions automatically when provided with audio signals from the vehicle driver, hence, introducing more automaton in the control-center. 

Our system consists of two sub-systems, as shown in figure \ref{Fig:10}, the first sub-system is the \ac{ASR} unit, which takes as input raw speech waveforms produced by the driver, and produces the information in a text form. The text is then passed to a trained text classifier, which given the text information from the \ac{ASR}, issues the corresponding appropriate dispatcher action.

\section{Automatic Speech Recognition Unit} 
\label{meth:s2}

One of the most crucial choices made was the decision about the \ac{ASR} system since high quality \ac{ASR} is a chief requirement for speech-based applications. We began by searching for open-source free German \ac{ASR}s with acceptable performance. Though many freely available state-of-the-art \ac{ASR} systems were found, they were mostly English \ac{ASR}s; this is mainly due to the availability of open-source English training data and the lack of other languages data available for free. Despite being rare, we found an interesting open-source German speech recognition model \cite{milde2018open} with freely available training recipes, open source training data, and pre-trained models ready for download and use. This model is based on Kaldi toolkit \cite{daniel2011kaldi} which is an open-source toolkit for speech recognition research. This model \cite{milde2018open} uses \ac{GMM} - \acf{HMM} and \ac{TDNN} \cite{waibel1990readings} \cite{peddinti2015time}, following the TED-LIUM corpus recipe example \cite{rousseau2014enhancing} in Kaldi. The model \cite{milde2018open} was trained on the German subset of the \ac{SWC} dataset\footnote{\url{https://en.wikipedia.org/wiki/Wikipedia: WikiProject_Spoken_Wikipedia}} (285 hours), Tuda-De dataset\footnote{\url{http://speech.tools/kaldi_tuda_de/german-speechdata-package-v2.tar.gz}} \cite{radeck2015open} (160 hours) and M-AILABS dataset\footnote{\url{https://www.caito.de/2019/01/the-m-ailabs-speech-dataset/}} (237 hours). For now, we only indicate the number of hours for each dataset, however, we shall discuss these datasets in details in section SECTION. The model therefore was trained on a total of 630 hours, and achieved $14.78\%$ \ac{WER} on the dev set of Tuda-De (11.5 hours) and $15.87\%$ on the test set of it (11.9 hours).


The above model \cite{milde2018open} is referred to as \enquote{Hybrid Model}, making use of both \ac{HMM}s and neural networks. We found no open-source end-to-end German \ac{ASR}, and despite the discussed model \cite{milde2018open} having satisfying performance, the idea of an end-to-end German \ac{ASR} was more attractive. This is because end-to-end systems have a much simpler training procedure, as many hand-engineered components and sophisticated pipelines are replaced by neural networks. End-to-end speech recognition is a fast-paced developing area of study with results improving continually. The end-to-end vision liberates us from domain-specific knowledge required for alignment, \ac{HMM} design, etc. And since end-to-end \ac{ASR} systems require little task specific components, an end-to-end English \ac{ASR} can be easily adapted to another language. A further advantage for end-to-end \ac{ASR}s is that they are more robust to noise and variation in speech and requires no special handling for them. In light of the great advantages for end-to-end systems, it was decided to find a suitable end-to-end \ac{ASR} that could be adapted to German without much complexity, and to use the Kaldi based model \cite{milde2018open} for comparative purposes only.



One seemingly convenient model was Deep Speech 2 \cite{amodei2016deep}, demonstrated in section \ref{bg:sub8}. The authors of the paper used their model for both English and Mandarin, two widely different languages. We took that as a proof of concept that the model could be easily adapted with very little changes to other languages including German. We started by mining for open source freely available German speech datasets that could be used since end-to-end \ac{ASR} systems require large amounts of transcribed audio data. We list the datasets found, their description, problems encountered while dealing with them and the cleaning procedures.


\subsection{Speech Recognition Datasets}
\label{meth:sub1}

For the purpose of unification, all

\subsubsection{\RomanNumeralCaps{1}. Common Voice}
\label{meth:subsubsub1}

Common Voice is the largest open source, multi-language dataset of voices available for use. It is managed by \href{https://www.mozilla.org/en-US/}{Mozilla} and was collected by volunteers on-line who were either recording samples of audio or validating other samples. Mozilla began work on this project in 2017 and contribution to the dataset continues up till now. Since most of the data used by large companies isn't available freely to researchers, Mozilla's aim was to create a free public dataset to make speech recognition open and accessible to everyone.
For our \ac{ASR}, the German subset of the dataset was selected. It incorporates 340 total hours, with 325 validated hours and 15 invalidated hours which were excluded. The dataset has 5007 speakers but as we discarded the invalidated utterances we end up with only 4823 speakers. 
All the utterances were in \enquote{mp3} format and sampled using a sampling rate of $44 kHz$ so we converted them to \enquote{wav} format and we performed down-sampling to obtain sample rate of $16 kHz$. We checked for any corrupted files but there were none.


\subsubsection{\RomanNumeralCaps{2}. M-AILABS Speech Dataset}
\label{meth:subsub2}
M-AILABS Speech Dataset is an open-source multi-lingual dataset provided by Munich Artificial Intelligence Laboratories GmbH. Most of the data is based on LibriVox \footnote{\url{https://librivox.org}} and Project Gutenberg \footnote{\url{https://www.gutenberg.org}}. We make use of the German subset which is 237 hours 22 minutes with a total of 5 speakers. The data is available in \enquote{wav} format and sample rate of $16 kHz$ so we perform no modifications. We also check for corrupted files but all of them were healthy.

\subsubsection{\RomanNumeralCaps{3}. German Speech Data \cite{radeck2015open}}
\label{meth:subsub3}

This open-source corpus is provided by Technische Universit{\"a}t Darmstadt. It has 36 hours read by 180 speakers, and recorded using 5 different microphones simultaneously. They made use of the KisRecord \footnote{\url{http://kisrecord.sourceforge.net}} toolkit, which allows for recording with multiple microphones concurrently. Their target was distant speech recognition, thus a distance of one meter between speakers and microphones was chosen. The sentences which volunteers were provided to read were extracted randomly from three text resources: German Wikipedia, German section of the European Parliament transcriptions, short commands for command-and-control settings. Unfortunately, there were some corrupted files in the dataset, which we discarded.

\subsubsection{\RomanNumeralCaps{4}. German Single Speaker Data}
\label{meth:subsub6}
hi

\subsubsection{\RomanNumeralCaps{3}. Movies Data}
\label{meth:subsub7}
hi

\subsubsection{Dataset Splitiing: Test, Dev, Train}
\label{meth:subsub7}
TODO SPEAKER INDEPENDENT CLASSIFICATION


Many open source implementations for the model were found that could be used such as \footnote{\url{https://github.com/noahchalifour/baidu-deepspeech2}} \footnote{\url{https://github.com/SeanNaren/deepspeech.pytorch}} \footnote{\url{https://github.com/tensorflow/models/tree/master/research/deep_speech}}. On the first attempt, we decided to use the \href{https://www.tensorflow.org/}{TensorFlow} \cite{tensorflow2015-whitepaper} implementation available in the TensorFlow Github repository \url{https://github.com/tensorflow/models/tree/master/research/deep_speech}. TensorFlow is an open source library for numerical computation used for machine applications. It was developed and maintained by \href{https://ai.google/research/teams/brain}{Google Brain}. At the beginning it was intended for internal use only, however, on  November 9, 2015, it was released under the \href{http://www.apache.org/licenses/LICENSE-2.0}{Apache License 2.0}.

One of the earliest problems encountered was our inability to run the model locally, as it is a large model demanding very high computational resources. As a result, we turned to \href{https://colab.research.google.com/}{Google Colab}, which is a free cloud platform used for machine learning education and research. It offers researchers the chance to run their applications using either a free GPU, or a cloud \ac{TPU}. A \ac{TPU} is an \ac{ASIC} optimized for performing high speed addition and multiplication operations, it was developed by Google for the purpose of speeding up machine learning applications. The implementation used TensorFlow \href{https://www.tensorflow.org/guide/estimators}{Estimators}, which is a high-level TensorFlow API that facilitates machine learning programming. Estimators encapsulate training, evaluation and prediction. They provide a training loop that controls building the graph, initializing variables, loading data, handling exceptions and creating checkpoints. For using Estimators, the data input pipeline should be separated from the model, thus making the model easier to work with different datasets. There are two types of Estimators: regular estimators, which work for both GPUs and CPUs, and \href{URL}{TPU Estimators}, which work for Google's cloud \ac{TPU}s. Since the implementation used normal estimators, we decided to run on Google Colab using a GPU which was a Tesla K80 with 12 GB memory.


As an initial experiment to make sure everything works, we used only the Tuda-De data, and used a model of 5 layers and 800 hidden size. After few days, the training reached for 3 epochs and a \ac{WER} of $80\%$. The issue however was clear, it was infeasible to proceed. The training was extremely time consuming and it would take months for the model to converge. We tried to make the model's size smaller and used 3 layers with hidden size of 700. With this setting and using Tuda-De only, the model converged at $50\%$ \ac{WER} after more than a week. We made sure the model was working and able to learn, however, the training speed was considerably slow and it was infeasible to train on the whole datasets we collected.


In search of other alternatives, we tried to make use of \href{https://www.tensorflow.org/tfrc}{TensorFlow Research Cloud Program}. This program offers researchers free cloud \ac{TPU}s in order to run their machine learning applications. We were offered $5$ v2 cloud \ac{TPU}s, each \ac{TPU} had $8$ cores with each core having $8$ GiB of \ac{HBM}. The \ac{TPU}s were available for use on \href{https://cloud.google.com/}{Google Cloud Platform}. The major obstacle was that the implementation used regular estimators; to overcome this it was necessary to migrate from normal estimators to \ac{TPU} estimators. This process was a simple one that required little code changes. The main issue, however, was that models are compiled using \href{https://www.tensorflow.org/xla/}{\ac{XLA}} when using \ac{TPU}s. \ac{XLA} is a compiler that requires that tensor dimensions must be statically defined at compile time. The problem was that not all utterances had the same number of frames, and we could not set a specific length to pad up to it and truncate smaller than it. The problem with truncating is that if we truncate after $n$ frames, how do we determine the labels to discard after the $n^{th}$ frame? The only solution was to pad the frames with zeros up to the maximum sequence length in the dataset, in this case, Tuda-De. After padding to the maximum number of frames, the \ac{TPU}s ran out of memory. After searching for solutions to this problem, the only solution found was to upgrade to TPU-v3 which 16 GiB of \ac{HBM} for each \ac{TPU} core, or to use a \ac{TPU} pod, which is a multiple \ac{TPU} devices connected together and the workloads distributed across all devices. Both options were infeasible for us.


In searching for other options, it was finally decided to abandon the TensorFlow implementation due to the problem of the \ac{TPU}s requiring tensors of static dimensions. We tried to use the PyTorch implementation of the model \url{https://github.com/SeanNaren/deepspeech.pytorch}. 
\href{https://pytorch.org/}{PyTorch} is an open source machine learning library which was developed by 
Facebook's artificial intelligence research group. It is based on Torch library and was released under the \href{https://en.wikipedia.org/wiki/BSD_licenses#3-clause}{ Modified BSD license}. In order to make sure the model was working and able to learn, we also used only Tuda-De data, with model of 3 layers and hidden size of 700 nodes. The training was started on Google Colab using a Tesla K80 GPU with 12 GB memory. The model was gradually improving, however it was also considerably slow.

At this point it was obvious there is a computational resources problem; Colab was very slow to use as it only used one GPU, and using the TPUs was not attainable. A cloud platform was needed in order to run the model and be able to iterate and experiment faster. For this purpose, we made us of Microsoft Azure's NC-series virtual machines. NC-series \ac{VM}s are armed with NVIDIA Tesla K80 GPUs and Intel Xeon E5-2690 v3 (Haswell) processors.
We chose Standard NC12 size, with 12 CPUs and a total of 112 GiB memory. It has also 2 GPUs with GPU memory of 24 GiB and SSD storage of 680 GiB.
All of our previous experiments were using \ac{GRU}s as the hidden unit of the \ac{RNN} layers. We decided to run two instances of training: one using 3 layers, 700 nodes, and \ac{GRU}s. The other one using 3 layers, 700 nodes and \ac{LSTM}s. After few epochs, the \ac{LSTM} instance was performing much better, however we did not proceed with training. At this point, we made sure the model was working and we got acceptable computational resources to run the training. In order to be able to use all of the datasets for the training, we upgraded the \ac{VM} to Standard NC24. This type has 24 CPUs and a total of 224 GiB memory, 4 GPUs with GPU memory of 48 GiB and SSD storage of 1440 GiB.


\subsubsection{problem of adding swc}
We were faced with a problem when trying to use the German \ac{SWC} dataset. The utterances were whole articles of very long durations that the memory would not fit them, even with the batch size set to 1. The dataset came with a segmentation file that included the alignments for the sentences. We used this segmentation file to cut the long wav files into small ones, however, after manually analyzing a random sample of the generated files, a high percentage of it was found to be very faulty. Since we have no method of aligning the data ourselves like in \cite{amodei2016deep}, we refrained from using the \ac{SWC} as we suspected it would deteriorate the overall performance.

We started the training with model size 5 layers, 800 nodes, and we added all the datasets except \ac{SWC}, using \ac{LSTM} hidden units. The training procedure took 10 days, through which we ran 19 epochs. It should be noted that after epoch 13, we used the noise augmentation option available and implemented in the pytorch model. It applies some changes to the tempo and gain when it's loading the audio. This aims at increasing robustness to environmental noise.

After 19 epochs, the model converged at \ac{WER} 29.571 and \ac{CER} 7.844 on our test set.  The fact that ctc adopt the assumption that the output labels are conditionally independent often makes the outputs suffer from errors that needs lingustic information to be corrected. In attempts to achieve better results, three techniques were investigated, and we discuss them in the following two subsections:

\subsubsection{Language Model Decoding}

In speech recognition, it is convenient to couple the model with a language model decoding technique in hope for achieving better \ac{WER}. Since there is more text data available more than transcribed audio, we can train a language model on massive amount of text corpora to generate a powerful n-gram language model. For this task, we make use of \href{https://kheafield.com/code/kenlm/}{KenLM}, which is a Language Model Toolkit that is easy to use and can be used to generate n-grams language models of any order. The target was to collect abundant text corpora to experiment with language models. The datasets we found and used are listed here:

\begin{enumerate}
	\item \textbf{German Wikipedia} \\
	We made use of the available \href{https://dumps.wikimedia.org/dewiki/latest/}{German Wikipedia dump} available for free download. The wikipedia was downloaded as one 20 GB xml file. We used a WikiExtractor library\footnote{\url{https://github.com/attardi/wikiextractor}} to extract the wikipedia into text files, which gave us a total of 56 folders, each containing 100 text files. Since KenLM requires that text data be in one file, with one sentence per line, we used spaCy\footnote{\url{https://spacy.io/}} which is a Python library for \ac{NLP} tasks. It supports many languages including German, we make use of the \href{https://spacy.io/models/de}{German model} and run it on all of the wikipedia text files. In order for the corpus to match the output of the \ac{ASR}, we perform the same text cleaning performed on the \ac{ASR} transcriptions: lowering all the letters, replacing numbers with their written form \textit{i.e.} 139 is changed into einhundertfUnfunddreissig, removing all punctuations and limiting all the vocab to letters from a to z plus the German umlauts A O U. All the B was replaced with ss and all the foreign characters were stripped out as well.
	
	
	
	\item \textbf{5 Million Web Sentences} \\
	We also make use of the Leipzig Corpora Collection offered by Universitat Leipzig. We make use of the data available in web section, and collect a total of 5Million web sentences. The data was found to be one sentence per line and need not further processing other than the same text cleaning performed on the Wikipedia.
	
	
	\item \textbf{8 Million Mary Sentences} \\
	We make use of the data used for language modeling by \ref{milde2018open}, available for download here\footnote{\url{ http://speech.tools/kaldi_tuda_de/German_sentences_8mil_filtered_maryfied.txt.gz}}. The data was in the needed format, with one sentence per line and needed no further processing.
	
	
	\item \textbf{Our transcriptions} \\
	We also make use of the transcriptions for our collected audio datasets. It was already one sentence per line and cleaned and suitable for use without any further processing.

	
\end{enumerate}

All of our data was augmented in one text file. We started by installing KenLM dependencies, and then installing KenLM itself. We perform word tokenization on our data using \href{https://www.nltk.org/}{nltk} library, then train an n-gram model with Kneser-Ney smoothing using KenLM. The generated output is a .arpa file which has a data section with unigram, bigram,..., n-gram counts followed by the estimated values. Though the .arpa format is more readable, the .binary format is much faster and more flexible. It remarkably reduces the loading time and the Language model is in fact required to be in .binary format to be used with our model. For these reasons, we binarize the model and obtain the final .binary labguage model file used for decoding. It should be noted that language model decoding with beam search is given by equation \ref{eq:25}

\begin{equation}
Q(y) = \log(p_{RNN}(y|x)) + \alpha \log(p_{LM}(y)) + \beta wc(y)
\end{equation}

where $wc(y)$ is the number of words in the transcription y. The weight $\alpha$ favors the outputs of the language model, while the weight $\beta$ encourages more words in the transcription.

We experiment with many data combinations and different values for $n$ which we report in details in the results section, however, our best result was $15.587\%$ \ac{WER} and $5.806\%$ \ac{CER} when using all our text corpora, with a 5-gram model and beam-with value of 500. The alpha and beta values were set to 0.9 and 0.2 respectively.

\subsubsection{Language Model Rescoring}

It is common as well to arm the model with language model rescoring technique which performs recognition in two passes. In the first pass, a relatively small language model with a low value of $n$ is used \textit{i.e.} 3 or 4 gram; this often makes the decoding process much simpler. We use N-best methode where the N-best scoring hypotheses outputs from the small language model are re-scored using a larger language model of higher $n$ value \textit{i.e.} 5 or 6 gram. The best scoring hypothesis according to the larger language model is then chosen to be the predicted transcription.
We expriment with various values for $n$ for the smaller language model and the larger one. The different settings with their respective results are reported in details in the results section \ref{chap:results}, howver, our best \ac{WER} was $x \%$ and was achieved with a small language model of $x$-gram built from all our text corpora, and a large language model of $x$-gram built as well from all the text corpora. We used a beam-width of $x$ value, and we set alpha to 0.9, and beta to 0.2.


\subsubsection{Auto Correct With Transformer}

In this experiment, we employ the method suggested in \cite{zhang2019automatic} by implementing an auto correct model that automatically corrects errored outputs of the ctc-based network. The main idea is to train a model which takes as input at inference the predictions generated by the \ac{ASR} model, and outputs the correct transcriptions. We stick to the model proposed in the paper \cite{zhang2019automatic} which is the Transformer \cite{vaswani2017attention}, however, we use the Transformer model implemented in \href{https://github.com/tensorflow/tensor2tensor}{tensor2tensor} library. \ac{Tensor2Tensor} is a machine learning library developed and maintained by \href{https://ai.google/research/teams/brain}{Google Brain} that aims at making machine learning research accessible and easier for researchers.
We start off by installing the library and then turn to defining our problem. The library's general interface is to define a problem that can be solved using different models and/or hyper-parameters. There are pre-defined problems in the library, however, we define our own and we call asr\_correction problem. Our problem is very similar to the translation problems defined in the library. We experiment with different datasets and hyper-parameters. For the training data, we use the movies data REFERENCE SECTION and the single speaker data REFERENCE as these two datasets were only added after epoch 13 in the asr, hence we can use the model's output at epoch 13 to transcribe these utterances. The training examples are made by pairing each transcribed output from the \ac{ASR} with its ground truth. When transcribing these utterances using greedy decoding, we obtain a total of 75K training examples.  For the sake of increasing the number of training examples, we use a $x$-gram model with beam-width $x$, 0.9 alpha and 0.2 beta, and get the best 10 outputs, and pair these outputs with the corresponding ground truth. This technique increases our training examples by a factor of 10 resulting in 750K training examples. We experiment with two hyperparameter sets of the transformer: transformer\_base\_single\_gpu and transformer\_big. We train both instances for $x$ steps, using the 750K examples. The transformer\_big achieved $x\%$ \ac{WER}, and $x\%$ \ac{CER}, while the transformer\_base\_single\_gpu  achieved $x\%$ \ac{WER}, and $x\%$ \ac{CER}. We also train the transformer\_big using the 75K training examples, however, more data got us an increase in performance by $x\%$.



%The CTC loss function (Graves et al., 2006) coupled with an RNN to model temporal information also performs well in endto-end speech recognition with character outputs (


%Due to the large amount of speakers and the diversity
%of topics included in the training data, our model is robust
%against speaker variation and topic shift.

For the purpose of comparing our system with the hybrid kaldi-based model \cite{milde2018open} illustrated in SECTION. We attempt to test the kaldi-based model on our test-set. We start by installing kaldi, and this \href{https://github.com/alumae/kaldi-gstreamer-server}{Kaldi GStreamer server} which is a real-time full-duplex speech recognition server. It is implemented in Python and based on GStreamer framework and Kaldi toolkit. Our test set is transcribed using the Kaldi GStreamer server and Kaldi's best model\footnote{\url{http://ltdata1.informatik.uni-hamburg.de/kaldi_tuda_de/de_400k_nnet3chain_tdnn1f_2048_sp_bi.tar.bz2}}. Then the output transcriptions are compared with the ground truth transcriptions in order to compute the \ac{WER} and the \ac{CER}. After comparing the transcriptions, we got $x\%$ \ac{WER} and  $x\%$ \ac{CER} on our dataset. This means that our best model outperforms the kaldi-based model with $x\%$ improvement in \ac{WER} and $x\%$ improvement in \ac{CER}

We also noticed that Kaldi-based model was tested on Tuda-De dataset excluding the realteck microphone since it's very low quality. We excluded all utterances recorded using this microphone, and tested our best model on the modified Tuda-De's test set. Our best model was after 19 epochs of training and coupled with a 5-gram language model decoding technique, beam-width value of x, using alpha and beta values of 0.9 and 0.2 respectively. We achieved $x\%$ \ac{WER} and $x\%$ \ac{CER} outperforming the kaldi-based model on this test set as well.



\section{Text Classifier Unit} 
\label{meth:s3}

TODO IN THE CHOICE OF ARCHITECTURE NLP PROBLEMS TO MAKE AN END TO END SYSTEM


\subsection{Text Classifier Datasets}
\label{meth:sub2}

\subsubsection{\RomanNumeralCaps{1}. German Wikipedia Dump}
\label{meth:subsub4}
\subsubsection{\RomanNumeralCaps{2}. 10K German Articles}
\label{meth:subsub5}

